<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>DMH Analytics</title>
    <link>/</link>
      <atom:link href="/index.xml" rel="self" type="application/rss+xml" />
    <description>DMH Analytics</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Tue, 21 Sep 2021 00:00:00 +0000</lastBuildDate>
    <image>
      <url>img/map[gravatar:%!s(bool=false) shape:circle]</url>
      <title>DMH Analytics</title>
      <link>/</link>
    </image>
    
    <item>
      <title>Connecting to Snowflake With R</title>
      <link>/post/connecting-to-snowflake-with-r/</link>
      <pubDate>Tue, 21 Sep 2021 00:00:00 +0000</pubDate>
      <guid>/post/connecting-to-snowflake-with-r/</guid>
      <description>&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;nav id=&#34;TableOfContents&#34;&gt;
  &lt;ul&gt;
    &lt;li&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#usecases&#34;&gt;Usecases&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#how-to-use-the-container&#34;&gt;How to use the container&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#rounding-things-up&#34;&gt;Rounding things up&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
  &lt;/ul&gt;
&lt;/nav&gt;
&lt;h3 id=&#34;introduction&#34;&gt;Introduction&lt;/h3&gt;
&lt;p&gt;As my organization 
&lt;a href=&#34;https://www.lederne.dk/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Lederne&lt;/a&gt; are in a journey of switching from on-prem systems into 
&lt;a href=&#34;https://www.snowflake.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Snowflake&lt;/a&gt; as our dataplatform, I have looked far and wide in order to find a production ready docker image that supports using R with 
&lt;a href=&#34;https://www.snowflake.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Snowflake&lt;/a&gt;. As I could not find anything in my search, I decided to build an image myself. Thankfully a lot of the driver setup was already build for a Python image created by 
&lt;a href=&#34;https://github.com/zoharsan/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Zohar Nissare-Houssen&lt;/a&gt; in his 
&lt;a href=&#34;https://github.com/zoharsan/snowflake-jupyter&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;snowflake-jupyter&lt;/a&gt; repo.&lt;/p&gt;
&lt;h3 id=&#34;usecases&#34;&gt;Usecases&lt;/h3&gt;
&lt;p&gt;I use R for a lot of data-wrangling, but also to help me with a lot of Data Engineering tasks. With R connected to snowflake, I have been able to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Speed up the writing of some SQL queries by utilizing the DBPlyr package&lt;/li&gt;
&lt;li&gt;Create and clean tables&lt;/li&gt;
&lt;li&gt;Schedule tasks that builds models from SnowfLake&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;how-to-use-the-container&#34;&gt;How to use the container&lt;/h3&gt;
&lt;p&gt;The container is open for everyone to get at 
&lt;a href=&#34;https://github.com/dannymawani/snowflake-r-docker&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;My github page&lt;/a&gt;. Below I have written how to use the image:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;To build the image after pulling it from GitHub, write &lt;code&gt;docker build --tag dev:latest .&lt;/code&gt; in your terminal where you have changed the directory to the folder the docker file is in.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Write &lt;code&gt;docker run -d -p 8787:8787  -e PASSWORD=1234 dev:latest&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;(replace pass to your liking). If you want, you can mount a directory with a project to work on like this:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;docker run --rm -p 8787:8787 -v C:/Users/DannysComputer/Documents/rstudio:/home/rstudio -e PASSWORD=1234 dev:latest&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;In order to just execute a script you can use:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-docker&#34;&gt;docker run dev:latest Rscript -e &#39;source(&amp;quot;/home/rstudio/main.R&amp;quot;)&#39; 
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;You can now log in at
http://localhost:8787/&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;connect-to-snowflake&#34;&gt;Connect to snowflake&lt;/h4&gt;
&lt;p&gt;Use this to set up a connection to Snowflake:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#Load libraries
library(tidyverse)
library(odbc)
library(DBI)
#log in
con &amp;lt;- DBI::dbConnect(
  drv    = odbc::odbc(), 
  UID    = &amp;quot;username&amp;quot;, 
  PWD    = &amp;quot;password&amp;quot;, 
  Server = &amp;quot;yourAccount.west-europe.azure.snowflakecomputing.com&amp;quot;,
  Warehouse = &#39;COMPUTE_WH&#39;,
  Driver = &amp;quot;SnowflakeDSIIDriver&amp;quot;,
  Database = &amp;quot;yourDataBase&amp;quot;,
  Schema = &amp;quot;yourSchema&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Alternatively, you can edit the odbc.ini file included in this repo and connect by running:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;con &amp;lt;- dbConnect(odbc(), &amp;quot;snowflake&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;using-snowflake-with-r-and-tidyverse-commands&#34;&gt;Using snowflake with R and Tidyverse commands&lt;/h4&gt;
&lt;p&gt;Below is some examples of things you can do after you have set up your connection with Snowflake&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#Connect to a table or view
df &amp;lt;- tbl(con, &#39;yourTableName&#39;)

#Full load table
data &amp;lt;- df%&amp;gt;%collect

#Load 10 rows
data &amp;lt;- head(10)%&amp;gt;%collect

#add table
dbCreateTable(con, &amp;quot;iris&amp;quot;, iris)

#remove table
dbRemoveTable(con, &amp;quot;iris&amp;quot;)

#upload data
dbWriteTable(con, &amp;quot;iris&amp;quot;, iris, overwrite = TRUE, row.names = FALSE)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;rounding-things-up&#34;&gt;Rounding things up&lt;/h3&gt;
&lt;p&gt;There are so many ways to interact with Snowflake. It is definitely one of my favorite platforms to work on at the moment. I personally enjoy using R for a variety of tasks, and having build this image has made things a bit easier for me. I hope this can be of use to others in the R commmunity using Snowflake.&lt;/p&gt;
&lt;p&gt;Let me know if this post is easy to follow or if it needs additional explanation by leaving a comment.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Make sure your data is up to date in bigquery</title>
      <link>/post/bigquery-monitoring-with-metadata/</link>
      <pubDate>Fri, 28 Feb 2020 00:00:00 +0000</pubDate>
      <guid>/post/bigquery-monitoring-with-metadata/</guid>
      <description>&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;nav id=&#34;TableOfContents&#34;&gt;
  &lt;ul&gt;
    &lt;li&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#the-service-account-key&#34;&gt;The service account key&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#building-the-code&#34;&gt;Building the code&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#optional-r-markdown&#34;&gt;Optional R Markdown&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#conclusion-and-final-thoughts&#34;&gt;Conclusion and final thoughts&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
  &lt;/ul&gt;
&lt;/nav&gt;
&lt;h3 id=&#34;introduction&#34;&gt;Introduction&lt;/h3&gt;
&lt;p&gt;In my work, data pipelines are nesscesary in order to provide clients with the correct data. unfortunately sometimes due to changes in the API, access being removed, or unexpected timeouts and crashes ends up having the data not being updated properly meaning that the marketing automation systems and dashboards are not updated.&lt;/p&gt;
&lt;p&gt;We almost store all relevant data in BigQuery, while the same data can be pushed to many other platforms such marketing automation systems, SQL databases etc. Because of this, we can utilize the metadata from BigQuery and see if the datasets are updated at the right time.&lt;/p&gt;
&lt;p&gt;At the moment the data is compiled to an R markdown document which makes anyone in our BI team to easily see if the data is up to date, should something be wrong, but use it in any way you see fit.&lt;/p&gt;
&lt;h3 id=&#34;the-service-account-key&#34;&gt;The service account key&lt;/h3&gt;
&lt;p&gt;I could spend a few hours writing down how to create a service account key in the Google Cloud Platform, however I think it would be smarter to link it right here instead: 
&lt;a href=&#34;https://cloud.google.com/iam/docs/creating-managing-service-account-keys&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://cloud.google.com/iam/docs/creating-managing-service-account-keys&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Once it is created, put it in the directory you are working on in R, and open it up. It should look like this:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;{
  &amp;quot;type&amp;quot;: &amp;quot;service_account&amp;quot;,
  &amp;quot;project_id&amp;quot;: &amp;quot;youproject&amp;quot;,
  &amp;quot;private_key_id&amp;quot;: &amp;quot;keyid1+239123&amp;quot;,
  &amp;quot;private_key&amp;quot;: &amp;quot;-----BEGIN PRIVATE KEY-----\nadfsdfsdfdsfdsfsdfaenIkd\nTVaeeeOZIODyN3ZYWxLwersffsdfdsb4we71lbHrg\nkJwHn8osrhssgfhCxp8y15YV8jrDof+TChTDe5wIj5WPvJO8\ndyBFchZx0ptbakmPb/ybeU7gKi3yPO29Mgoz4Cb47gNPvuZX82ic3dYE7YMEO1VF\nyfc1zHF6UJEAdrMSe+YO5oiGABemGj8FTNb+0Q0XurC4da3Qvwmz43OaICNTGzgI\nmkHB6fqsvSSVDi0agPE68wVGIDSEew9kEGUeAMw/e+T7NIY8z2PYJ5Dv6RWj/+b9\nh1XHiEwBAgMBAAECggEAISrTAqBfXFX966X8CVPjd3C198Yn/oyMssdfdsfds√ÖQelFF\nJKxAt/Pnyr2xDYZLWgK3QChYibhufCwHq6V7BiWSO7F2PluuJedr9scG7u+t5se7\nr7gpyhTZIxljRsfdgsfgmEGDQz1wVpYnghySNHIHSWQcRG\nARYkv9v9LmwStA20Tr3BGEa1t3GF+4DAnW5DIP50TwzHv0wiG0+5vX3bWc+x/NR0\njAJKsDcoMzNtIqieLm7cizjS9Ku+rdHaelN8dt51jVAHATQ4oXwEyA0SpbkaLbqx\nvU+Y4U3plPZ1uAWHAtSI6fsrqOKpWA3g6MHVDiJQKQKBgQDGqbi62wq6vWETzByo\nOoiVvJLszm8iYA7LkgKPsJsfVvd+HJcv7uldL04ZaUIGvZ24shbRLN78Z0AnRzxX\nXZF3Vn64ROfxDSkcYOitipRdxBlsdfgrfbbZqeOvPVSy3qMn1Bqlf\ndFNvEtcMQ6YhRbAjbjgvlgiBTQKBgQC6yseYbmiJyRASBcyXcaguuvoebnn7l8r8\n2S1A7wTt1Sy5iPn1RfmUP4DK09y4zNtRYtNwNhu/VhB20MU8Na9ImKBVuvA3O0Ic\nIUeRFgDI2M3u4rGkLlsfgdgdf7f9oRUhPefGIAcNiqm3SACB2w9T\nuo5yMGIbhQKBgQCSkR7wBLCqysfdgfdgsdfO1X6U4kvCyG8oV1chsdfgfdgO6sSveufbbYGApKhpy2R6fydxNQKBgBHg\n√ò√ò√ò√ò√òkzEA1exc3HfaaB5ayGz5LvLKP5hwe2Lt47HL6TFg\nw9t5Ui59jhXQ5Q9eE+hesvmBanver/UBOlgoQur11L+H8nhD9vEQOLOThyA2xmha\nS6V0sacUQ3Fq+0N6zTH+√•√•√•√•√•√∏√∏√∏√•√•+q4y7JQSQiHd23\n22e4PAfX3Df+8860Axrx6fkImbvKxuOZ+uYpwhYMpPdgvukuAqXCHDcG5Fiho28g\n27JHFzQql0pn9Pj4O+WkUOs0rX9rcPjvV9h6Jc/YEWV6YwXMnEkKRaMj44+x7a69\npPfrTO7pBJAquiiU3dCmVDU=\n-----END PRIVATE KEY-----\n&amp;quot;,
  &amp;quot;client_email&amp;quot;: &amp;quot;yourserviceaccoutname@yourproject.iam.gserviceaccount.com&amp;quot;,
  &amp;quot;client_id&amp;quot;: &amp;quot;12345678910&amp;quot;,
  &amp;quot;auth_uri&amp;quot;: &amp;quot;https://accounts.google.com/o/oauth2/auth&amp;quot;,
  &amp;quot;token_uri&amp;quot;: &amp;quot;https://oauth2.googleapis.com/token&amp;quot;,
  &amp;quot;auth_provider_x509_cert_url&amp;quot;: &amp;quot;https://www.googleapis.com/oauth2/v1/certs&amp;quot;,
  &amp;quot;client_x509_cert_url&amp;quot;: &amp;quot;https://www.googleapis.com/robot/v1/metadata/x509/yourserviceaccountname%40yourproject.iam.gserviceaccount.com&amp;quot;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here you need to add the email you see under &lt;code&gt; &amp;quot;client_email&amp;quot;: &amp;quot;yourserviceaccoutname@yourproject.iam.gserviceaccount.com&amp;quot;&lt;/code&gt; and add that to all the BigQuery projects you want to monitor:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;iamaccess.png&#34; alt=&#34;IAM ACESS&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;building-the-code&#34;&gt;Building the code&lt;/h3&gt;
&lt;h5 id=&#34;1-authenticate-and-load-libraries&#34;&gt;1. authenticate and load libraries**&lt;/h5&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#libraries
library(bigQueryR)
library(purrr)
library(dplyr)

#auth
bigQueryR::bqr_auth(&amp;quot;yourauthfile.json&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;2-get-a-list-of-all-projects-and-datasets&#34;&gt;2. Get a list of all projects and datasets&lt;/h5&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#get projects
projects &amp;lt;- bigQueryR::bqr_list_projects()
#get datasets
datasets &amp;lt;- lapply(projects$projectId,bqr_list_datasets)
datasets &amp;lt;- bind_rows(datasets)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This will give us two tables that show us all the projects and datasets that the service account have access to. We need this information to list all avaliable tables.&lt;/p&gt;
&lt;h5 id=&#34;3-get-a-list-of-all-tables-to-extract-metadata&#34;&gt;3. Get a list of all tables to extract metadata&lt;/h5&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#get tables
listTables &amp;lt;- function(id,dataset){
bqr_list_tables(projectId = id,
                datasetId = dataset, maxResults = -1)
}

tables &amp;lt;- mapply(listTables,datasets$projectId,datasets$datasetId)
tablesCombined &amp;lt;- bind_rows(tables)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then we can use these lines in order to extract the metadata:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#get meta
tablesMeta &amp;lt;- function(project,dataset,table){
bqr_table_meta(projectId = project,
               datasetId = dataset, table)
}

tablesMeta &amp;lt;- mapply(tablesMeta,tablesCombined$projectId,tablesCombined$datasetId,tablesCombined$tableId)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It can take some time if you have a lot of projects. If you want to speed up the process you can process it with the future package instead to run it in parallel: 
&lt;a href=&#34;https://cran.r-project.org/web/packages/future/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://cran.r-project.org/web/packages/future/index.html&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Once it is done, we get a list of which gives us a list of objects with metadata:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;bqmeta.png&#34; alt=&#34;BQ META&#34;&gt;&lt;/p&gt;
&lt;p&gt;As seen, it isn&amp;rsquo;t great for working with yet, so we need to take the information we need for each list object and stitch it together in a format that is more fit for working with.&lt;/p&gt;
&lt;p&gt;The process will be to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Get the number of rows in the listobject&lt;/li&gt;
&lt;li&gt;create an empty dataframe with column names&lt;/li&gt;
&lt;li&gt;extract the data we need for each list&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;#create a dataframe with the right information for bq
#get the total rows
i &amp;lt;- seq_along(1:NROW(tablesMeta)) 
#create an empty dataframe with column headers
bqUpdate &amp;lt;- setNames(data.frame(matrix(ncol = 7, nrow = 0)), c(&amp;quot;id&amp;quot;,&amp;quot;projectId&amp;quot;,&amp;quot;datasetId&amp;quot;,&amp;quot;creationTime&amp;quot;,&amp;quot;lastModifiedTime&amp;quot;,&amp;quot;type&amp;quot;,&amp;quot;location&amp;quot;))

#create function for looping for appending metadata
combineMetaData &amp;lt;- function(i){
add_row(bqUpdate,
       id = tablesMeta[[i]][[&amp;quot;id&amp;quot;]],
       projectId = tablesMeta[[i]][[&amp;quot;tableReference&amp;quot;]][[&amp;quot;projectId&amp;quot;]],
       datasetId = tablesMeta[[i]][[&amp;quot;tableReference&amp;quot;]][[&amp;quot;datasetId&amp;quot;]],
       creationTime = tablesMeta[[i]][[&amp;quot;creationTime&amp;quot;]],
       lastModifiedTime = tablesMeta[[i]][[&amp;quot;lastModifiedTime&amp;quot;]],
       type = tablesMeta[[i]][[&amp;quot;type&amp;quot;]],
       location = tablesMeta[[i]][[&amp;quot;location&amp;quot;]]
       )
}

#run the function and combine it
upload &amp;lt;- map(i,combineMetaData)
upload &amp;lt;- bind_rows(upload)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally format the table a little, so it is more readable:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;upload$creationTime &amp;lt;- format(as.POSIXct(as.double(upload$creationTime)/1000, origin = &amp;quot;1970-01-01&amp;quot;, tz = &amp;quot;GMT-1&amp;quot;),&amp;quot;%Y-%m-%d %H:%M:%S&amp;quot; )
upload$lastModifiedTime &amp;lt;- format(as.POSIXct(as.double(upload$lastModifiedTime)/1000, origin = &amp;quot;1970-01-01&amp;quot;, tz = &amp;quot;GMT-1&amp;quot;),&amp;quot;%Y-%m-%d %H:%M:%S&amp;quot; )
upload$dataset &amp;lt;- gsub(&amp;quot;.*\\.&amp;quot;,&amp;quot;&amp;quot;,upload$id) 
upload$daysSinceUpdate &amp;lt;- Sys.Date()-as.Date(as.character(upload$lastModifiedTime), format=&amp;quot;%Y-%m-%d&amp;quot;)
upload &amp;lt;- upload[order(upload$daysSinceUpdate, decreasing=F),]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The output should look something like this:
&lt;img src=&#34;bqjobdone.png&#34; alt=&#34;bqtableextracted&#34;&gt;&lt;/p&gt;
&lt;p&gt;And voila, you now have a list of all your tables in BigQuery, as well as being able to see how many days it is since they were last updated.&lt;/p&gt;
&lt;h3 id=&#34;optional-r-markdown&#34;&gt;Optional R Markdown&lt;/h3&gt;
&lt;p&gt;In our company, the persons in charge of marketing automation and data visualization, wanted to be able to pinpoint if there should be any issues with having updated data in their systems, as the first thing they look through. I therefore created a 
&lt;a href=&#34;https://rmarkdown.rstudio.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;R markdown document&lt;/a&gt;, that they could use by visiting a page stored in 
&lt;a href=&#34;https://cloud.google.com/storage/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Google Cloud Storage&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;mqmd.png&#34; alt=&#34;bqmd&#34;&gt;&lt;/p&gt;
&lt;p&gt;This makes it possible to see the tables who are not updated within the last day, which one who are updated, and a list of all tables. It even colors table in different colors depending on how long it has been since the last update.&lt;/p&gt;
&lt;p&gt;Tables not updated are not nesscesarily a bad thing, as some static tables are needed to benchmark. I do however recommend to look at some of the tables once in a while to see if it would make sense to delete them.&lt;/p&gt;
&lt;p&gt;Below is the code that I used to generate it:&lt;/p&gt;
&lt;script src=&#34;https://gist.github.com/dannymawani/a38d2a5717a89e7218f6d91c7f465bdc.js&#34;&gt;&lt;/script&gt;
&lt;h3 id=&#34;conclusion-and-final-thoughts&#34;&gt;Conclusion and final thoughts&lt;/h3&gt;
&lt;p&gt;This post displays a way to monitor your bigQuery tables. If you are running a lot of jobs, it can make sense to use this as a quick way to see if some tables are not updated, should some of your systems fail. For building data pipelines using cronjobs, og cloud builds etc. I do recommend take a look at 
&lt;a href=&#34;https://cloud.google.com/products/operations&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Stackdriver in GCP&lt;/a&gt;, as it can monitor failed jobs and send textmessages, slack notifications or emails with customized dynamic text. I will try and write a guide about this in the future.&lt;/p&gt;
&lt;p&gt;If you do consider automating grabbing the metadata from bigQuery, I recommend you to read 
&lt;a href=&#34;https://code.markedmondson.me/googleCloudRunner/articles/usecases.html#build-an-rmd-on-a-schedule-and-host-its-html-on-cloud-storage&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Mark&amp;rsquo;s post on how to update it automatically using Google Cloud Build&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Let me know if this post is easy to follow or if it needs additional explanation by leaving a comment.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Detect missing data in GA with R</title>
      <link>/post/measurement-protocol-usecases/</link>
      <pubDate>Fri, 12 Jul 2019 00:00:00 +0000</pubDate>
      <guid>/post/measurement-protocol-usecases/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;This post is based on a project where we needed to see how many transactions we were actually missing in Google Analytics.&lt;/p&gt;
&lt;h2 id=&#34;the-how-to&#34;&gt;The how to&lt;/h2&gt;
&lt;p&gt;This process is quite simple,
we will be pulling out transaction id&amp;rsquo;s and then make it into a list. After that we compare that list with another&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ga_id &amp;lt;- &amp;quot;yourgaid&amp;quot;

data &amp;lt;- google_analytics(ga_id, 
                         date_range = c(&amp;quot;2019-01-23&amp;quot;, &amp;quot;2019-02-13&amp;quot;), 
                         metrics = &amp;quot;users&amp;quot;, 
                         dimensions = c(&amp;quot;transactionId&amp;quot;),
                         max = -1)

#Your transaction ids
rowa &amp;lt;- as.list(data$transactionId)

#the other list of transaction ids
rowb &amp;lt;- as.list(sfData$ti)

#make them into characters
rowa &amp;lt;- as.character(rowa)
rowb &amp;lt;- as.character(rowb)

#Only get the ones that doesn&#39;t match
list1 &amp;lt;- setdiff(rowb, rowa)

#Make a dataframe from only those who are missing
df &amp;lt;- sfData[sfData$ti %in% list1, ]
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;This is a very short tutorial, it is not formatted very pretty, but it should get the trick done in terms of validating if anything should be missing in terms of transactions in Google Analytics. Please leave a comment should you need further elaboration on this progress!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>R on the Google Cloud Platform - Scheduling tasks</title>
      <link>/talk/r-on-google-cloud-platform/</link>
      <pubDate>Sat, 08 Jun 2019 00:00:00 +0000</pubDate>
      <guid>/talk/r-on-google-cloud-platform/</guid>
      <description>&lt;p&gt;&lt;strong&gt;the slides&lt;/strong&gt;&lt;/p&gt;
&lt;div style=&#34;position: relative; padding-bottom: 80.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;/slides/r-on-gcp-slides&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; webkitallowfullscreen mozallowfullscreen allowfullscreen&gt;&lt;/iframe&gt;
 &lt;/div&gt;</description>
    </item>
    
    <item>
      <title>Creating a workflow with measurement protocol and R</title>
      <link>/post/detect-missing-data-with-r-a-google-analytics-example/</link>
      <pubDate>Fri, 01 Mar 2019 00:00:00 +0000</pubDate>
      <guid>/post/detect-missing-data-with-r-a-google-analytics-example/</guid>
      <description>&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;nav id=&#34;TableOfContents&#34;&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#how-the-measurement-protcol-works&#34;&gt;How the measurement protcol works&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#usecases&#34;&gt;Usecases&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#uploading-refunds&#34;&gt;Uploading refunds&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#uploading-transactions&#34;&gt;Uploading transactions&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#words-of-caution&#34;&gt;words of caution&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#1-create-filters-to-block-views&#34;&gt;1. Create filters to block views&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#2-remove-the-exclude-all-hits-from-known-bots-and-spiders-checkmark&#34;&gt;2. Remove the &amp;ldquo;Exclude all hits from known bots and spiders&amp;rdquo; checkmark&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#3-latency&#34;&gt;3. Latency&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#conlusion&#34;&gt;conlusion&lt;/a&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/nav&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;This blogpost was inspired by a release to a website which had some last minute changed that when the site when live, caused a javaScript error that blocked the ecommerce transaction data to not populate.&lt;/p&gt;
&lt;p&gt;With this error, the client lost 1800+ transactions to the site meaning that we had to upload these in the right currency format to make sure that all the data was still avaliable.&lt;/p&gt;
&lt;h2 id=&#34;how-the-measurement-protcol-works&#34;&gt;How the measurement protcol works&lt;/h2&gt;
&lt;p&gt;Whenever you visit a website, Google Analytic will collect information by sending a long list of query parameters to an endpoint, to which it is stored and modelled to show information in GA. Visiting this blogpost will for an example generate a network call similar to this:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-javaScript&#34;&gt;https://www.google-analytics.com/r/collect?v=1&amp;amp;_v=j73&amp;amp;a=1679354847&amp;amp;t=pageview&amp;amp;_s=1&amp;amp;dl=https%3A%2F%2Fwww.mawani.dk%2Fpost%2Fmeasurement-protocol-usecases%2F&amp;amp;ul=en-us&amp;amp;de=UTF-8&amp;amp;dt=Creating%20a%20workflow%20with%20measurement%20protocol%20and%20R%20%7C%20DMH%20Analytics&amp;amp;sd=24-bit&amp;amp;sr=1440x900&amp;amp;vp=1440x256&amp;amp;je=0&amp;amp;_u=QACAAAAB~&amp;amp;jid=651243071&amp;amp;gjid=1319476248&amp;amp;cid=1164110852.1529450664&amp;amp;tid=UA-134673438-1&amp;amp;_gid=106302445.1550868348&amp;amp;_r=1&amp;amp;gtm=2wg241WBTJ7HR&amp;amp;z=1265093896
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This will tell Google Analytics that:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The URL is this one&lt;/li&gt;
&lt;li&gt;My browser language is en-us&lt;/li&gt;
&lt;li&gt;what my device id is from the cookie so it can recognize me as a new or returning user&lt;/li&gt;
&lt;li&gt;What tracking ID it should populate the data to&lt;/li&gt;
&lt;li&gt;What my session ID is.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;What measurement protocol will do is to emulate the same types of parameters and have it populate the data in GA.&lt;/p&gt;
&lt;p&gt;For a more detailed view on measurement protocol I would advise you to visit

&lt;a href=&#34;https://www.optimizesmart.com/understanding-universal-analytics-measurement-protocol/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Optimize Smart for their deepgoing description&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;usecases&#34;&gt;Usecases&lt;/h2&gt;
&lt;p&gt;I have used measurement protocol for a various number of things. As you can send in all the supported information, this is a great area to explore in terms of improving your data collection. As for many of my other posts, I will be using another package created by 
&lt;a href=&#34;https://twitter.com/HoloMarkeD&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Mark Edmonson&lt;/a&gt; called 
&lt;a href=&#34;https://github.com/MarkEdmondson1234/googleMeasureR&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;googleMeasureR&lt;/a&gt; to send the hits to Google Analytics, however you can easily transfer the way of doing this to any language of your choice such as Python, javaScript, PHP etc.&lt;/p&gt;
&lt;h3 id=&#34;uploading-refunds&#34;&gt;Uploading refunds&lt;/h3&gt;
&lt;p&gt;To upload refunds you first need a list of all the transactions that are missing. If you are on a site with multiple currencies it is also quite important that you can specify what unit that you are adding data to, so you are sure that it is populated correctly.&lt;/p&gt;
&lt;p&gt;First thing that need to be done is to make sure that the data is loaded into R. I have chosen to rename the columns to the GA naming convention for measurement protocol, but it is not important:&lt;/p&gt;





  











&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;/img/transaction_example.png&#34; &gt;


  &lt;img src=&#34;/img/transaction_example.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;Once that is made, we need to either have the client id (Google Analytics Cookie ID for identifying people), or create one yourself. This can be done by this code:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;
#loading the library
library(stringi)
#the count of rows you need to upload
n &amp;lt;- nrow(yourDataframe)

#create the ids
paste(&amp;quot;1.2.&amp;quot;,stri_rand_strings(n, 10, pattern = &amp;quot;[0-9]&amp;quot;),&amp;quot;.&amp;quot;,stri_rand_strings(n, 10, pattern = &amp;quot;[0-9]&amp;quot;), sep = &amp;quot;&amp;quot;)

#output will look something like this if you print it out in the console

 [1] &amp;quot;1.2.6237558844.6715115260&amp;quot; &amp;quot;1.2.4893337628.0287915875&amp;quot; &amp;quot;1.2.1414860945.6106576917&amp;quot;
 [4] &amp;quot;1.2.8992586956.6450277842&amp;quot; &amp;quot;1.2.2032023475.7135641438&amp;quot; &amp;quot;1.2.1490282359.1139988407&amp;quot;
 [7] &amp;quot;1.2.7613957961.8692578524&amp;quot; &amp;quot;1.2.8481705025.8828767075&amp;quot; &amp;quot;1.2.1331455569.2063291053&amp;quot;
[10] &amp;quot;1.2.9292335596.4531869013&amp;quot;

### add it to the dataframe

yourDataframe$cid &amp;lt;- paste(&amp;quot;1.2.&amp;quot;,stri_rand_strings(n, 10, pattern = &amp;quot;[0-9]&amp;quot;),&amp;quot;.&amp;quot;,stri_rand_strings(n, 10, pattern = &amp;quot;[0-9]&amp;quot;), sep = &amp;quot;&amp;quot;)

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now that a client id is added we are ready to push the refunds into Google Analytics:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;
####upload
v &amp;lt;- 1 #version
cs &amp;lt;- &amp;quot;measurementprotocol&amp;quot; #source
cm &amp;lt;- &amp;quot;refund&amp;quot; #medium
t &amp;lt;- &amp;quot;event&amp;quot; #type
ec &amp;lt;- &amp;quot;Ecommerce&amp;quot; #event category
ea &amp;lt;- &amp;quot;measurementProtocol&amp;quot; #event action
el &amp;lt;- &amp;quot;refund&amp;quot; #event label
tid &amp;lt;- &amp;quot;UA-1234567-1&amp;quot; #Google Analytics ID
pa &amp;lt;- &amp;quot;refund&amp;quot; #product action
ta &amp;lt;- &amp;quot;measurementprotocol&amp;quot; #transaction affiliation

#for each row in the dataframe send a hit with measurement protocol
for(i in 1:nrow(yourDataframe)) {
  cid &amp;lt;- yourDataframe$cid[i]
  ti &amp;lt;- yourDataframe$ti[i]
  tr &amp;lt;- yourDataframe$tr[i]
  cu &amp;lt;- yourDataframe$cu[i]
  gmr_post(list(v=v,cs=cs,cm=cm,t=t,ec=ec,ea=ea,el=el,tid=tid,cid=cid,pa=pa,ti=ti,tr=tr,cu=cu,ta=ta,ni=&amp;quot;1&amp;quot;))

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And there you have it, the refunds should be uploaded.&lt;/p&gt;
&lt;h3 id=&#34;uploading-transactions&#34;&gt;Uploading transactions&lt;/h3&gt;
&lt;p&gt;This is actually the exact same steps you go through - the primary difference is that you need to change the product action from refund to purchase:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;
####upload
v &amp;lt;- 1 #version
cs &amp;lt;- &amp;quot;measurementprotocol&amp;quot; #source
cm &amp;lt;- &amp;quot;purchase&amp;quot; #medium
t &amp;lt;- &amp;quot;event&amp;quot; #type
ec &amp;lt;- &amp;quot;Ecommerce&amp;quot; #event category
ea &amp;lt;- &amp;quot;measurementProtocol&amp;quot; #event action
el &amp;lt;- &amp;quot;refund&amp;quot; #event label
tid &amp;lt;- &amp;quot;UA-1234567-1&amp;quot; #Google Analytics ID
pa &amp;lt;- &amp;quot;purchase&amp;quot; #product action
ta &amp;lt;- &amp;quot;measurementprotocol&amp;quot; #transaction affiliation

#for each row in the dataframe send a hit with measurement protocol
for(i in 1:nrow(yourDataframe)) {
  cid &amp;lt;- yourDataframe$cid[i]
  ti &amp;lt;- yourDataframe$ti[i]
  tr &amp;lt;- yourDataframe$tr[i]
  cu &amp;lt;- yourDataframe$cu[i]
  gmr_post(list(v=v,cs=cs,cm=cm,t=t,ec=ec,ea=ea,el=el,tid=tid,cid=cid,pa=pa,ti=ti,tr=tr,cu=cu,ta=ta,ni=&amp;quot;1&amp;quot;))

&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;words-of-caution&#34;&gt;words of caution&lt;/h2&gt;
&lt;p&gt;When you add data to Google Analytics, it is not possible to remove it again, so remember to test what you are doing beforehand and be sure that what you are doing is correct.&lt;/p&gt;
&lt;p&gt;Below is some areas that you need to remember when doing uploads through measurement protocol:&lt;/p&gt;
&lt;h3 id=&#34;1-create-filters-to-block-views&#34;&gt;1. Create filters to block views&lt;/h3&gt;
&lt;p&gt;Create an exclude filter to exclude all event action called &amp;ldquo;measurementprotocol&amp;rdquo;. This ensures that you data is only send to the views you need.&lt;/p&gt;
&lt;h3 id=&#34;2-remove-the-exclude-all-hits-from-known-bots-and-spiders-checkmark&#34;&gt;2. Remove the &amp;ldquo;Exclude all hits from known bots and spiders&amp;rdquo; checkmark&lt;/h3&gt;
&lt;p&gt;In some instances I saw that the measurement protocol hits where blocked when this was not checked off. Remember to switch it on the day after so you are secured against people spamming your analytics account with bot traffic.&lt;/p&gt;
&lt;h3 id=&#34;3-latency&#34;&gt;3. Latency&lt;/h3&gt;
&lt;p&gt;There is a latency from the the data is send to that it is populated. Unfortunately if you add / remove filters before the day is over it will take in / remove the hits from measurement protocol meaning that you will endanger your datacollection.&lt;/p&gt;
&lt;h2 id=&#34;conlusion&#34;&gt;conlusion&lt;/h2&gt;
&lt;p&gt;This is a simple how-to guide on doing basic things with measurement protocol, there are a lot about datacleaning that could have been included but it would be out of the scope of this blogpost.&lt;/p&gt;
&lt;p&gt;In terms of all the parameters added in the call, there are many that could have been excluded, however i choose to add them as it will be easier to filter them out in different instances should you need to do that.&lt;/p&gt;
&lt;p&gt;A main issue with measurement protocol, when you don&amp;rsquo;t have the right client id is that you do loose the user behaviour and affect your traffic if you are doing bulk uploads in a day. It is therefore important to consider if this is the right solution for the task, and be sure that the data being send in is correct.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Pull data from multiple Google Analytics Properties with R</title>
      <link>/post/pull-data-from-multiple-ga-properties/</link>
      <pubDate>Fri, 01 Mar 2019 00:00:00 +0000</pubDate>
      <guid>/post/pull-data-from-multiple-ga-properties/</guid>
      <description>&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;nav id=&#34;TableOfContents&#34;&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;a href=&#34;#background&#34;&gt;Background&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#usecase&#34;&gt;Usecase&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#the-code&#34;&gt;The code&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#1-connect-to-google-analytics&#34;&gt;1. Connect To Google Analytics&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#2-setting-up-the-script-to-pull-from-multiple-properties&#34;&gt;2. Setting up the script to pull from multiple properties&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#conclusion&#34;&gt;Conclusion&lt;/a&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/nav&gt;
&lt;h2 id=&#34;background&#34;&gt;Background&lt;/h2&gt;
&lt;p&gt;Being in an agency, we often have to do benchmarks, reports etc. that require us to pull data from multiple Google Analytics properties.&lt;/p&gt;
&lt;h2 id=&#34;usecase&#34;&gt;Usecase&lt;/h2&gt;
&lt;p&gt;One of our client have multiple properties each containing 3 business units, a B2B, B2C and B2G (Business to government). To make sure that we could reproduce the reporting for them, we build an R script that:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Pulls Google Analytics data&lt;/li&gt;
&lt;li&gt;Adds business unit and country to the dataframe as additional variables / columns&lt;/li&gt;
&lt;li&gt;Merges the data into another dataframe&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;the-code&#34;&gt;The code&lt;/h2&gt;
&lt;p&gt;To extract the data, use the &lt;code&gt;googleAnalyticsR&lt;/code&gt;and &lt;code&gt;googleAuthR&lt;/code&gt;package made by 
&lt;a href=&#34;https://twitter.com/HoloMarkeD&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Mark Edmonson&lt;/a&gt;. To see more information about installing the packages and using the libraries please check out the site made explaining how the package work 
&lt;a href=&#34;http://code.markedmondson.me/googleAnalyticsR/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;1-connect-to-google-analytics&#34;&gt;1. Connect To Google Analytics&lt;/h3&gt;
&lt;p&gt;To begin with we must first authenticate to Google Analytics.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(googleAnalyticsR)
googleAnalyticsR::ga_auth()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next you will be asked to log in with your Google Account. Once that is done, we are ready to do the rest of the script.&lt;/p&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Remember to authenticate with the account you want access to
  &lt;/div&gt;
&lt;/div&gt;
&lt;h3 id=&#34;2-setting-up-the-script-to-pull-from-multiple-properties&#34;&gt;2. Setting up the script to pull from multiple properties&lt;/h3&gt;
&lt;p&gt;To combine different properties we are fest specifying the views we need and adding them to a list. Once that is done we will create 2 corresponding lists with business units and countries.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#this are the view links which can be find under view settings or through the URL of the view - If you are used to working with this package, you can also do an extraction of all your views directly from R. The below views are fake.

#Danish
dk_b2b &amp;lt;- &amp;quot;123213213&amp;quot;
dk_b2c &amp;lt;- &amp;quot;543454533&amp;quot;
dk_b2g &amp;lt;- &amp;quot;173714215&amp;quot;

#Finnish
fi_b2b &amp;lt;- &amp;quot;345345435&amp;quot;
fi_b2c &amp;lt;- &amp;quot;345435345&amp;quot;
fi_b2g &amp;lt;- &amp;quot;234234232&amp;quot;

#French
fr_b2b &amp;lt;- &amp;quot;655464555&amp;quot;
fr_b2c &amp;lt;- &amp;quot;989834589&amp;quot;
fr_b2g &amp;lt;- &amp;quot;039485309&amp;quot;

#adding all the views to a list
views &amp;lt;- c(dk_b2b,dk_b2c,dk_b2g,fi_b2b,fi_b2c,fi_b2g,fr_b2b,fr_b2c,fr_b2g)

#countries should be in the same order as your list above, we will use this to add attributes to the dataframe
countries &amp;lt;- c(&amp;quot;DK&amp;quot;,&amp;quot;DK&amp;quot;,&amp;quot;DK&amp;quot;,&amp;quot;FI&amp;quot;,&amp;quot;FI&amp;quot;,&amp;quot;FI&amp;quot;,&amp;quot;FR&amp;quot;,&amp;quot;FR&amp;quot;,&amp;quot;FR&amp;quot;)

#same approach for business units
BU &amp;lt;- c(&amp;quot;B2B&amp;quot;,&amp;quot;B2C&amp;quot;,&amp;quot;B2G&amp;quot;,&amp;quot;B2B&amp;quot;,&amp;quot;B2C&amp;quot;,&amp;quot;B2G&amp;quot;,&amp;quot;B2B&amp;quot;,&amp;quot;B2C&amp;quot;,&amp;quot;B2G&amp;quot;)

#Set a start and enddate
startDate &amp;lt;- &amp;quot;2018-01-01&amp;quot;
endDate &amp;lt;- as.character(Sys.Date()-1)

#Set dimensions and metrics
dimensions &amp;lt;- c(&amp;quot;year&amp;quot;,&amp;quot;sourceMedium&amp;quot;,&amp;quot;campaign&amp;quot;)
metrics &amp;lt;- c(&amp;quot;sessions&amp;quot;,&amp;quot;transactions&amp;quot;,&amp;quot;transactionRevenue&amp;quot;)

#create an empty dataframe
upload &amp;lt;- data.frame()

#pulling the data
for(i in seq_along(views)) {
  data &amp;lt;- google_analytics(views[i], date_range = c(startDate,endDate), metrics = metrics,
                   dimensions = dimensions,max = -1)

data$country &amp;lt;- countries[i]
data$bu &amp;lt;- BU[i]

upload &amp;lt;- rbind(upload,data)

}

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The end states that for each element in the view list, then run the analytics script, apply the data and business unit to the corresponding list item to the dataframe &amp;ldquo;data&amp;rdquo;, and then add it to the empty dataframe.&lt;/p&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Since writing this post i now recommend using mapply (or future_mapply to run it faster in parallel). I will however not change this, as the for loop might do a better job explaining what happens and how the process work.
  &lt;/div&gt;
&lt;/div&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Now that the dataframe has been made it is up to you what to do with it. you can either upload it to 
&lt;a href=&#34;https://bigquery.cloud.google.com&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;bigQuery&lt;/a&gt;, do some statistics with R and some plot with 
&lt;a href=&#34;https://ggplot2.tidyverse.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ggplot2&lt;/a&gt; or just write it down to a CSV file with &lt;code&gt;r write.csv2(upload,&amp;quot;yourFileName.csv&amp;quot;)&lt;/code&gt; and work with it in another tool like Excel, Tableau or powerBI.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Source R scripts from github through Google Cloud Storage</title>
      <link>/post/source-r-with-github/</link>
      <pubDate>Fri, 01 Mar 2019 00:00:00 +0000</pubDate>
      <guid>/post/source-r-with-github/</guid>
      <description>&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;nav id=&#34;TableOfContents&#34;&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;a href=&#34;#background&#34;&gt;Background&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#why-is-sourcing-from-github-smart&#34;&gt;Why is sourcing from github smart?&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#the-how-to-part&#34;&gt;The how-to part&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#1-setting-up-a-github-account&#34;&gt;1. Setting up a github account&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#2-creating-a-bucket-in-google-cloud-storage&#34;&gt;2. Creating a bucket in Google Cloud Storage&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#3-creating-a-trigger-with-cloud-build&#34;&gt;3. Creating a trigger with Cloud Build&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#4-setting-up-the-yalm&#34;&gt;4. Setting up the YALM&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#5-sourcing-scripts-from-cloud-storage&#34;&gt;5. Sourcing scripts from Cloud Storage&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#thoughts-and-conclusion&#34;&gt;Thoughts and conclusion&lt;/a&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/nav&gt;
&lt;p&gt;source-r-with-github&lt;/p&gt;
&lt;h2 id=&#34;background&#34;&gt;Background&lt;/h2&gt;
&lt;p&gt;In my company, we have gone from 1 person working with adhoc assignment with R, to 4 persons within the last year. This set&amp;rsquo;s up a whole new level of requirements to ensure quality, stability and safety when it comes to working with datamodels and API calls. Furthermore, there is also an aspect of reusability that needed to be set up to ensure that we could produce our solutions faster,and better updated.&lt;/p&gt;
&lt;p&gt;To do this, we started looking into differen&amp;rsquo;t types of systems to work with our code. In the end we have selected Github as our main source of creating our code and documentation.&lt;/p&gt;
&lt;p&gt;To get to know how to do this, I actually used this guide: 
&lt;a href=&#34;https://www.google.com&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Automated Static Website Publishing with Cloud Build
&lt;/a&gt;, and used most of the principles in the guide to set up our setup.&lt;/p&gt;
&lt;h2 id=&#34;why-is-sourcing-from-github-smart&#34;&gt;Why is sourcing from github smart?&lt;/h2&gt;
&lt;p&gt;Github allows us to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Share code easily&lt;/li&gt;
&lt;li&gt;Scale projects to other customers&lt;/li&gt;
&lt;li&gt;Ensure documentation&lt;/li&gt;
&lt;li&gt;Making updates to code safe and with the possibility to roll-back should something break&lt;/li&gt;
&lt;li&gt;See who have created code&lt;/li&gt;
&lt;li&gt;Source Scripts directly in R&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Unfortunately, going into how to use Github is a bit out of the scope for this post, however I recommend going to Github and read their documentation and do a few searches on the net to get started.&lt;/p&gt;
&lt;h2 id=&#34;the-how-to-part&#34;&gt;The how-to part&lt;/h2&gt;
&lt;p&gt;In order to source R scripts from github there are a few things you need to have ready in order to get ready:&lt;/p&gt;
&lt;h3 id=&#34;1-setting-up-a-github-account&#34;&gt;1. Setting up a github account&lt;/h3&gt;
&lt;p&gt;One of the first thing you will need is to actually create a 
&lt;a href=&#34;https://www.google.com&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github account&lt;/a&gt;
. This can be done on the frontpage:&lt;/p&gt;





  











&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;/img/githubFrontpage_githubR.png&#34; &gt;


  &lt;img src=&#34;/img/githubFrontpage_githubR.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;Once that is done, 
&lt;a href=&#34;https://github.com/new&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;create a repository&lt;/a&gt;:&lt;/p&gt;





  











&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;/img/createRepositories_githubR.png&#34; &gt;


  &lt;img src=&#34;/img/createRepositories_githubR.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;Recently, it&amp;rsquo;s been possible to 
&lt;a href=&#34;https://www.google.com&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;create private repositories for free&lt;/a&gt;&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;. As we are multiple users that are using Github right now, we did manage to get our financaial department to approve the monthly 7$ account fee, however in theory, you could create a shared login to help get you started.&lt;/p&gt;
&lt;h3 id=&#34;2-creating-a-bucket-in-google-cloud-storage&#34;&gt;2. Creating a bucket in Google Cloud Storage&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;a href=&#34;https://cloud.google.com/billing/docs/how-to/manage-billing-account&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Set up billing for GCP (Google Cloud Platform)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://console.cloud.google.com/storage/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Create a bucket in Google Cloud Storage)&lt;/a&gt;: &lt;br&gt;





  











&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;/img/createBucket_githubR.png&#34; &gt;


  &lt;img src=&#34;/img/createBucket_githubR.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;



&lt;/figure&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;3-creating-a-trigger-with-cloud-build&#34;&gt;3. Creating a trigger with Cloud Build&lt;/h3&gt;
&lt;p&gt;Now, this part is something that is already documented in the GCP documentaion. To avoid copying the same guide in this post, 
&lt;a href=&#34;https://cloud.google.com/community/tutorials/automated-publishing-cloud-build&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;instead go to this page&lt;/a&gt; and follow the steps from the headling: &lt;strong&gt;Set up automated builds&lt;/strong&gt; until you see this line: &amp;ldquo;Now, create a cloudbuild.yaml&amp;rdquo;. From there, you will not need to follow the rest of the steps to continue this guide.&lt;/p&gt;
&lt;h3 id=&#34;4-setting-up-the-yalm&#34;&gt;4. Setting up the YALM&lt;/h3&gt;
&lt;p&gt;YALM stands for &amp;ldquo;Yet Another Markup Language&amp;rdquo; and will be added to Github. From here, the YALM file sends the information to Cloud Build that copies the updates on Github into the Cloud Storage Bucket just created.&lt;/p&gt;
&lt;p&gt;In your Github repo, click on &amp;ldquo;Create new file&amp;rdquo;. Now, add the following YALM syntax in the edit section:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-YALM&#34;&gt;steps:
  - name: gcr.io/cloud-builders/gsutil
    args: [&amp;quot;-m&amp;quot;, &amp;quot;rsync&amp;quot;, &amp;quot;-r&amp;quot;, &amp;quot;-c&amp;quot;, &amp;quot;-d&amp;quot;, &amp;quot;.&amp;quot;, &amp;quot;gs://yourfolderincloudstorage/ifneededthencreateasubfolder&amp;quot;]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;save the file as cloudbuild.yaml.&lt;/p&gt;
&lt;p&gt;Once that is done, each change made to your repository will be pushed directly to Google Cloud Storage, where it is possible to run R scripts.&lt;/p&gt;
&lt;h3 id=&#34;5-sourcing-scripts-from-cloud-storage&#34;&gt;5. Sourcing scripts from Cloud Storage&lt;/h3&gt;
&lt;p&gt;Playing around with R and the Google API universe usually means using something that 
&lt;a href=&#34;http://code.markedmondson.me&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Mark Edmonson has build&lt;/a&gt;, and this post is not an exception.&lt;/p&gt;
&lt;p&gt;We will be using the Package called &amp;ldquo;googleCloudStorageR&amp;rdquo;, which you can find more information about 
&lt;a href=&#34;https://cran.r-project.org/web/packages/googleCloudStorageR/vignettes/googleCloudStorageR.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;To source a script from Google Cloud Storage, use this &lt;code&gt;R code&lt;/code&gt;to authenticate and source the script you need:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-R&#34;&gt;#install.package(&amp;quot;googleCloudStorageR&amp;quot;) if you have not installed it yet

#load the library
library(googleCloudStorageR)

#Authenticate
gcs_auth()

googleCloudStorageR::gcs_source(&#39;yoursubfolderifyouhaveone/yourscript.R&#39;, bucket = &#39;yourcreatedbucket&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And voila. This means that you can always download your R files where ever you need it, build on it, add the information back to github and source the script from a third place without having to have multiple copies of your code laying around.&lt;/p&gt;
&lt;h2 id=&#34;thoughts-and-conclusion&#34;&gt;Thoughts and conclusion&lt;/h2&gt;
&lt;p&gt;This post have showed how to use Github to run R scripts and keep them updated. I do recommend trying to document and automate as much as possible when working with R, however this should only be used whenever it makes sense. If you are building something on the fly that will only be used for a specific tasks once, then it might not make sense to go through all of this to make your workflow optimal.&lt;/p&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;A repository is a folder to which your content in github is stored. This is usually your code files.&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</description>
    </item>
    
    <item>
      <title>MEASURECAMP 2019 - CPH</title>
      <link>/slides/r-on-gcp-slides/</link>
      <pubDate>Tue, 05 Feb 2019 00:00:00 +0000</pubDate>
      <guid>/slides/r-on-gcp-slides/</guid>
      <description>&lt;h1 id=&#34;measurecamp-cph19&#34;&gt;Measurecamp CPH19&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;Creating datapipelines with R&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;By Danny Mawani Holmgaard&lt;/em&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;agenda&#34;&gt;Agenda&lt;/h2&gt;
&lt;div style=&#34;float: left; width: 50%; text-align: left;&#34;&gt;
&lt;ol&gt;
&lt;li&gt;About my journey&lt;/li&gt;
&lt;li&gt;Stuff I am still figuring out&lt;/li&gt;
&lt;li&gt;Learn to grab different types of data&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div style=&#34;float: right; width: 50%; text-align: left;&#34;&gt;
&lt;ol start=&#34;4&#34;&gt;
&lt;li&gt;Connect Github and Cloud Storage&lt;/li&gt;
&lt;li&gt;Use R in the GCP&lt;/li&gt;
&lt;li&gt;Extract Transform and Load data into BigQuery&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;h3 id=&#34;my-journey&#34;&gt;My journey&lt;/h3&gt;
&lt;div style=&#34;float: left; width: 50%; text-align: left;&#34;&gt;
&lt;ul&gt;
&lt;li&gt;Worked with Digital Analytics for 6 years&lt;/li&gt;
&lt;li&gt;Lead Analyst @ IMPACT EXTEND&lt;/li&gt;
&lt;li&gt;Responsible for all data pipelines&lt;/li&gt;
&lt;li&gt;Moving from digital analytics to data engineering&lt;/li&gt;
&lt;li&gt;Primarily use R and GCP&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div style=&#34;float: right; width: 50%; text-align: left;&#34;&gt;
&lt;p&gt;&lt;img src=&#34;superweek.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;h2 id=&#34;our-team&#34;&gt;Our team&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;workstuff.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;stuff-i-am-currently-trying-to-get-better-at-using&#34;&gt;Stuff I am currently trying to get better at using&lt;/h3&gt;
&lt;div style=&#34;float: left; width: 60%; text-align: left;&#34;&gt;
&lt;ul&gt;
&lt;li&gt;Using Python and SQL instead of using R for everything&lt;/li&gt;
&lt;li&gt;Utilizing Docker&lt;/li&gt;
&lt;li&gt;Set up plumber API&amp;rsquo;s to make my life easier&lt;/li&gt;
&lt;li&gt;Deploying shinyapps to GCP with authentication&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div style=&#34;float: right; width: 40%; text-align: left;&#34;&gt;
&lt;p&gt;&lt;img src=&#34;docker.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;h3 id=&#34;what-i-am-trying-to-say-is-that-there-is-a-lot-to-grasp-and-that-it-takes-time-to-optimize-your-workflow&#34;&gt;What i am trying to say is that there is a lot to grasp and that it takes time to optimize your workflow&lt;/h3&gt;
&lt;hr&gt;
&lt;h3 id=&#34;but-now-lets-get-to-the-place-where-we-can-get-to-do-some-cool-stuff&#34;&gt;But now, lets get to the place where we can get to do some cool stuff!&lt;/h3&gt;
&lt;hr&gt;
&lt;h1 id=&#34;grabbing-different-types-of-data&#34;&gt;Grabbing different types of data&lt;/h1&gt;
&lt;hr&gt;
&lt;h3 id=&#34;sql-servers&#34;&gt;SQL Servers&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#load libraries
#library(tidyverse)
#library(dbi)
#library(odbc)

#set up connection object
con &amp;lt;- DBI::dbConnect(odbc::odbc(),
                      Driver    = &amp;quot;SQL Server&amp;quot;, 
                      Server    = &amp;quot;yourserver.database.windows.net&amp;quot;,
                      Database  = &amp;quot;databasename&amp;quot;,
                      UID       = &amp;quot;userid&amp;quot;,
                      PWD       = &amp;quot;password&amp;quot;,
                      Port      = 1234)

#extract data
dataset &amp;lt;- as_tibble(
  tbl(con, &amp;quot;dataset&amp;quot;)%&amp;gt;%
  head(100) #Get the first 100 rows
  ) 

&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h3 id=&#34;ftp-servers&#34;&gt;FTP Servers&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#library(RCurl)
syntax &amp;lt;- &amp;quot;ftp://&amp;quot;
ftpHost &amp;lt;- &amp;quot;ftp.yourserver.com.com&amp;quot;
ftpUser &amp;lt;- &amp;quot;username&amp;quot;
ftpPass &amp;lt;- &amp;quot;password&amp;quot;
folder &amp;lt;- &amp;quot;/folder/&amp;quot; #delete folder if in root
ftpURL &amp;lt;- paste(syntax,ftpHost,folder,sep = &amp;quot;&amp;quot;)

#Download files function

download &amp;lt;- function(file){
  fileDownload &amp;lt;- file
  downloadFtpUrl &amp;lt;- paste(ftpURL,fileDownload, sep = &amp;quot;&amp;quot;)
  downloadFtpUrlCredentials &amp;lt;- paste(ftpUser,&amp;quot;:&amp;quot;,ftpPass, sep = &amp;quot;&amp;quot;)
  bin &amp;lt;- getBinaryURL(downloadFtpUrl,userpwd=downloadFtpUrlCredentials)
  con &amp;lt;- file(fileDownload, open = &amp;quot;wb&amp;quot;)
  writeBin(bin, con)
  close(con)}

#Download files

download(&amp;quot;yourfile.csv&amp;quot;)

&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;postgress-database&#34;&gt;postgress database&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#library(&#39;RPostgreSQL&#39;)

#create connection object
con &amp;lt;- dbConnect(drv =&amp;quot;RPostgreSQL&amp;quot;, 
                 user=&amp;quot;&amp;quot;, 
                 password=&amp;quot;&amp;quot;,
                 host=&amp;quot;&amp;quot;, 
                 port=1234, 
                 dbname=&amp;quot;&amp;quot;)

#extract data
dataset &amp;lt;- as_tibble(
  tbl(con, &amp;quot;dataset&amp;quot;)%&amp;gt;%
  head(100) #Get the first 100 rows
  ) 

&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;google-analytics&#34;&gt;Google Analytics&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;ga.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;facebook-and-instagram&#34;&gt;Facebook and Instagram&lt;/h3&gt;
&lt;div style=&#34;float: left; width: 50%; text-align: left;&#34;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;It is difficult to download Facebook and instagram data&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Seek a third party vendor instead&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div style=&#34;float: right; width: 50%; text-align: right;&#34;&gt;
&lt;ul&gt;
&lt;li&gt;Remember, maintaining API work can require a lof of work and could potentially break whereas a vendor are living of maintaining these tools and have the right types of access&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;p&gt;&lt;img src=&#34;stitch.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;connect-github-and-cloud-storage-to-your-work&#34;&gt;Connect Github and Cloud storage to your work&lt;/h2&gt;
&lt;hr&gt;
&lt;h3 id=&#34;github&#34;&gt;Github&lt;/h3&gt;
&lt;div style=&#34;float: left; width: 60%; text-align: right;&#34;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Github is the closest thing you get to a &amp;ldquo;dropbox&amp;rdquo; for your code.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;It allows version control and makes sure that your code is always updated&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;From github, you can push you code into other systems and work on the same projects&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div style=&#34;float: right; width: 40%; text-align: right;&#34;&gt;
&lt;p&gt;&lt;img src=&#34;github.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;h2 id=&#34;cloud-storage&#34;&gt;Cloud Storage&lt;/h2&gt;
&lt;div style=&#34;float: left; width: 60%; text-align: right;&#34;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Cloud storage lets you upload and pull files in a secure environment&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;It has great API&amp;rsquo;s and can sync directly with bigQuery&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Thanks to Mark Edmonson, we can also source R code directly from there&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div style=&#34;float: right; width: 40%; text-align: right;&#34;&gt;
&lt;p&gt;&lt;img src=&#34;cloudstorage.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div style=&#34;float: left; width: 50%; text-align: right;&#34;&gt;
&lt;ol&gt;
&lt;li&gt;Set up Github and put you code there&lt;/li&gt;
&lt;li&gt;Create a bucket in Cloud Storage&lt;/li&gt;
&lt;li&gt;Create a trigger with cloudbuild where you link to the repo&lt;/li&gt;
&lt;li&gt;Add cloudbuild.yalm to the folder&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div style=&#34;float: right; width: 50%; text-align: right;&#34;&gt;
&lt;pre&gt;&lt;code class=&#34;language-javaScript&#34;&gt;steps:
  - name: gcr.io/cloud-builders/gsutil
    args: [&amp;quot;-m&amp;quot;, &amp;quot;rsync&amp;quot;, &amp;quot;-r&amp;quot;, &amp;quot;-c&amp;quot;, &amp;quot;-d&amp;quot;, &amp;quot;.&amp;quot;, &amp;quot;gs://yourfolderincloudstorage/ifneededthencreateasubfolder&amp;quot;]
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;h2 id=&#34;sourcing-from-cloud-storage&#34;&gt;Sourcing from Cloud Storage&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;
#install.package(&amp;quot;googleCloudStorageR&amp;quot;) if you have not installed it yet

#load the library
library(googleCloudStorageR)

#Authenticate
gcs_auth()

googleCloudStorageR::gcs_source(&#39;yoursubfolderifyouhaveone/yourscript.R&#39;, bucket = &#39;yourcreatedbucket&#39;)
                 
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h1 id=&#34;use-r-studio-in-the-google-cloud-platform&#34;&gt;Use R studio in the Google Cloud Platform&lt;/h1&gt;
&lt;hr&gt;
&lt;h2 id=&#34;1-set-up-billing&#34;&gt;1. Set up billing&lt;/h2&gt;
&lt;hr&gt;
&lt;h2 id=&#34;2-thank-mark-that-he-build-a-script-that-does-everything-for-you&#34;&gt;2. Thank Mark that he build a script that does everything for you&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# You need to authenticate with your GCP account before being able to do it
gce_vm(&amp;quot;yourmachinename&amp;quot;, project =&amp;quot;gar-creds-185213&amp;quot;, zone = &amp;quot;europe-west1-b&amp;quot;,
       predefined_type = &amp;quot;g1-small&amp;quot;,
       template = &amp;quot;rstudio&amp;quot;, 
       username = &amp;quot;username&amp;quot;, 
       password = &amp;quot;password&amp;quot;)

&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;the-easy-way-to-run-scripts-automatically-from-gcs&#34;&gt;The easy way to run scripts automatically from GCS&lt;/h2&gt;
&lt;div style=&#34;float: left; width: 50%; text-align: right;&#34;&gt;
&lt;p&gt;&lt;img src=&#34;rstudiogcp.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div style=&#34;float: right; width: 50%; text-align: right;&#34;&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#set machine to be launched
library(googleComputeEngineR)
library(googleCloudStorageR)
library(googleAuthR)

gar_auth(&amp;quot;/home/username/.httr-oauth&amp;quot;)

GCE_AUTH_FILE=&amp;quot;/home/username/auth.json&amp;quot;
GCE_DEFAULT_PROJECT_ID=&amp;quot;projectname&amp;quot;
GCE_DEFAULT_ZONE=&amp;quot;europe-west1-b&amp;quot;
gcs_global_bucket(&amp;quot;bucketname&amp;quot;)
BQ_AUTH_FILE=&amp;quot;/home/username/bq.oauth&amp;quot;

vm &amp;lt;- gce_vm(&amp;quot;yourvirtualmachine&amp;quot;)

vm &amp;lt;- gce_ssh_setup(vm,
                    username = &amp;quot;username&amp;quot;,
                    key.pub = &amp;quot;/home/username/.ssh/id_rsa.pub&amp;quot;,
                    key.private = &amp;quot;/home/username/.ssh/id_rsa&amp;quot;)

runme &amp;lt;- &amp;quot;Rscript -e \&amp;quot;googleAuthR::gar_gce_auth();googleCloudStorageR::gcs_source(&#39;cloudstoragefolder/script.r&#39;, bucket = &#39;bucket&#39;)\&amp;quot;&amp;quot;
docker_cmd(vm, 
           cmd = &amp;quot;exec&amp;quot;, 
           args = c(&amp;quot;rstudio&amp;quot;, runme), 
           wait = TRUE,
           capture_text = FALSE)

&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;h3 id=&#34;automating-the-script-to-run-at-a-specific-time&#34;&gt;Automating the script to run at a specific time&lt;/h3&gt;
&lt;h4 id=&#34;remember---your-if-you-turn-of-the-machine-the-cronjob-settings-will-stop-working&#34;&gt;&lt;em&gt;Remember - your if you turn of the machine, the cronjob settings will stop working&lt;/em&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img src=&#34;cronjobs.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;sending-data-to-bigquery&#34;&gt;Sending data to bigQuery&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#library(bigQueryR)

# First Create the dataset

# bqr_create_table(projectId = &amp;quot;your project id&amp;quot;,
#                  datasetId = &amp;quot;dataset&amp;quot;, &amp;quot;your table&amp;quot;, your dataframe,
#                  timePartitioning = FALSE, expirationMs = 0L)

bqr_upload_data(projectId = &amp;quot;your project id&amp;quot;,
                datasetId = &amp;quot;dataset&amp;quot;, &amp;quot;your table&amp;quot;, yourdataframe,
                overwrite = FALSE, #True to overwrite your table
                wait = TRUE, autodetect = FALSE,
                maxBadRecords = 1000)

&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;final-thoughts&#34;&gt;Final thoughts&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;This is not the most stable way to do things, but the easiest&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;There is so many ways you can work with making your data flow&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Start small and build your way up from there&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;h2 id=&#34;questions&#34;&gt;Questions?&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;@dannymawani (Twitter / Linkedin)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;
&lt;a href=&#34;http://www.mawani.dk&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;www.mawani.dk&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;
&lt;a href=&#34;mailto:dmo@impact.dk&#34;&gt;dmo@impact.dk&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Superweek 2019 - A tale of automation in data engineering</title>
      <link>/talk/superweek-2019/</link>
      <pubDate>Fri, 01 Feb 2019 00:00:00 +0000</pubDate>
      <guid>/talk/superweek-2019/</guid>
      <description>&lt;p&gt;&lt;strong&gt;the slides&lt;/strong&gt;&lt;/p&gt;
&lt;iframe src=&#34;//www.slideshare.net/slideshow/embed_code/key/28qsnfrw9J3zHF&#34; width=&#34;595&#34; height=&#34;485&#34; frameborder=&#34;0&#34; marginwidth=&#34;0&#34; marginheight=&#34;0&#34; scrolling=&#34;no&#34; style=&#34;border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;&#34; allowfullscreen&gt; &lt;/iframe&gt; &lt;div style=&#34;margin-bottom:5px&#34;&gt; &lt;strong&gt; &lt;a href=&#34;//www.slideshare.net/DannyMawaniOlsen/superweek2019-dmo-presentation&#34; title=&#34;Superweek2019 dmo presentation&#34; target=&#34;_blank&#34;&gt;Superweek2019 dmo presentation&lt;/a&gt; &lt;/strong&gt; from &lt;strong&gt;&lt;a href=&#34;https://www.slideshare.net/DannyMawaniOlsen&#34; target=&#34;_blank&#34;&gt;Danny Mawani Olsen&lt;/a&gt;&lt;/strong&gt; &lt;/div&gt;</description>
    </item>
    
    <item>
      <title>Black friday seminar</title>
      <link>/talk/blackfriday/</link>
      <pubDate>Sat, 01 Dec 2018 10:00:00 +0000</pubDate>
      <guid>/talk/blackfriday/</guid>
      <description>&lt;h1 id=&#34;se-videoen-her&#34;&gt;Se videoen her:&lt;/h1&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/LNRdiEFOtMQ&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

</description>
    </item>
    
    <item>
      <title>GDPR with Google Tag Manager</title>
      <link>/talk/ip-anonymization-while-excluding-internal-trafic/</link>
      <pubDate>Sat, 01 Dec 2018 00:00:00 +0000</pubDate>
      <guid>/talk/ip-anonymization-while-excluding-internal-trafic/</guid>
      <description>&lt;iframe src=&#34;//www.slideshare.net/slideshow/embed_code/key/8L7n7nX0ZUINKB&#34; width=&#34;595&#34; height=&#34;485&#34; frameborder=&#34;0&#34; marginwidth=&#34;0&#34; marginheight=&#34;0&#34; scrolling=&#34;no&#34; style=&#34;border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;&#34; allowfullscreen&gt; &lt;/iframe&gt; &lt;div style=&#34;margin-bottom:5px&#34;&gt; &lt;strong&gt; &lt;a href=&#34;//www.slideshare.net/DannyMawaniOlsen/gdpr-within-google-tag-manager-measurecamp-2018&#34; title=&#34;GDPR within Google Tag Manager - Measurecamp 2018&#34; target=&#34;_blank&#34;&gt;GDPR within Google Tag Manager - Measurecamp 2018&lt;/a&gt; &lt;/strong&gt; from &lt;strong&gt;&lt;a href=&#34;https://www.slideshare.net/DannyMawaniOlsen&#34; target=&#34;_blank&#34;&gt;Danny Mawani Olsen&lt;/a&gt;&lt;/strong&gt; &lt;/div&gt;</description>
    </item>
    
    <item>
      <title>How to anonymize IP adresses and still be able to exclude internal traffic</title>
      <link>/talk/gdpr-with-google-tag-manager/</link>
      <pubDate>Sat, 01 Dec 2018 00:00:00 +0000</pubDate>
      <guid>/talk/gdpr-with-google-tag-manager/</guid>
      <description>&lt;p&gt;
&lt;a href=&#34;https://analytics.mawani.dk/excluding-internal-traffic-anonymizing-ip-adresses-google-analytics/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Original Link&lt;/a&gt;&lt;/p&gt;
&lt;iframe src=&#34;//www.slideshare.net/slideshow/embed_code/key/6Vr0jPcBgPD6dd&#34; width=&#34;595&#34; height=&#34;485&#34; frameborder=&#34;0&#34; marginwidth=&#34;0&#34; marginheight=&#34;0&#34; scrolling=&#34;no&#34; style=&#34;border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;&#34; allowfullscreen&gt; &lt;/iframe&gt; &lt;div style=&#34;margin-bottom:5px&#34;&gt; &lt;strong&gt; &lt;a href=&#34;//www.slideshare.net/DannyMawaniOlsen/anonymization-of-ip-adresses-with-google-tag-manager&#34; title=&#34;Anonymization of IP adresses with Google Tag Manager&#34; target=&#34;_blank&#34;&gt;Anonymization of IP adresses with Google Tag Manager&lt;/a&gt; &lt;/strong&gt; from &lt;strong&gt;&lt;a href=&#34;https://www.slideshare.net/DannyMawaniOlsen&#34; target=&#34;_blank&#34;&gt;Danny Mawani Olsen&lt;/a&gt;&lt;/strong&gt; &lt;/div&gt;</description>
    </item>
    
    <item>
      <title>R introduction class for measurecamp</title>
      <link>/talk/r-introduction-class-measurecamp/</link>
      <pubDate>Sat, 01 Dec 2018 00:00:00 +0000</pubDate>
      <guid>/talk/r-introduction-class-measurecamp/</guid>
      <description>&lt;p&gt;&lt;strong&gt;the slides&lt;/strong&gt;&lt;/p&gt;
&lt;iframe src=&#34;//www.slideshare.net/slideshow/embed_code/key/uz2aLKQKZ2CSiB&#34; width=&#34;595&#34; height=&#34;485&#34; frameborder=&#34;0&#34; marginwidth=&#34;0&#34; marginheight=&#34;0&#34; scrolling=&#34;no&#34; style=&#34;border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;&#34; allowfullscreen&gt; &lt;/iframe&gt; &lt;div style=&#34;margin-bottom:5px&#34;&gt; &lt;strong&gt; &lt;a href=&#34;//www.slideshare.net/DannyMawaniOlsen/rclass&#34; title=&#34;Rclass&#34; target=&#34;_blank&#34;&gt;Rclass&lt;/a&gt; &lt;/strong&gt; from &lt;strong&gt;&lt;a href=&#34;https://www.slideshare.net/DannyMawaniOlsen&#34; target=&#34;_blank&#34;&gt;Danny Mawani Olsen&lt;/a&gt;&lt;/strong&gt; &lt;/div&gt;
&lt;p&gt;
&lt;a href=&#34;https://documentcloud.adobe.com/link/track?uri=urn:aaid:scds:US:a2aa2406-42b7-4030-8bb9-fa53525d20c8&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;full link here&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>External Project</title>
      <link>/project/external-project/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      <guid>/project/external-project/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Internal Project</title>
      <link>/project/internal-project/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      <guid>/project/internal-project/</guid>
      <description>&lt;p&gt;Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.&lt;/p&gt;
&lt;p&gt;Nullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.&lt;/p&gt;
&lt;p&gt;Cras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.&lt;/p&gt;
&lt;p&gt;Suspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.&lt;/p&gt;
&lt;p&gt;Aliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
