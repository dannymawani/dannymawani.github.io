<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>DMH Analytics</title>
    <link>/authors/admin/</link>
      <atom:link href="/authors/admin/index.xml" rel="self" type="application/rss+xml" />
    <description>DMH Analytics</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Fri, 28 Feb 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>img/map[gravatar:%!s(bool=false) shape:circle]</url>
      <title>DMH Analytics</title>
      <link>/authors/admin/</link>
    </image>
    
    <item>
      <title>Make sure your data is up to date in bigquery</title>
      <link>/post/bigquery-monitoring-with-metadata/</link>
      <pubDate>Fri, 28 Feb 2020 00:00:00 +0000</pubDate>
      <guid>/post/bigquery-monitoring-with-metadata/</guid>
      <description>&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;nav id=&#34;TableOfContents&#34;&gt;
  &lt;ul&gt;
    &lt;li&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#the-service-account-key&#34;&gt;The service account key&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#building-the-code&#34;&gt;Building the code&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#optional-r-markdown&#34;&gt;Optional R Markdown&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#conclusion-and-final-thoughts&#34;&gt;Conclusion and final thoughts&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
  &lt;/ul&gt;
&lt;/nav&gt;
&lt;h3 id=&#34;introduction&#34;&gt;Introduction&lt;/h3&gt;
&lt;p&gt;In my work, data pipelines are nesscesary in order to provide clients with the correct data. unfortunately sometimes due to changes in the API, access being removed, or unexpected timeouts and crashes ends up having the data not being updated properly meaning that the marketing automation systems and dashboards are not updated.&lt;/p&gt;
&lt;p&gt;We almost store all relevant data in BigQuery, while the same data can be pushed to many other platforms such marketing automation systems, SQL databases etc. Because of this, we can utilize the metadata from BigQuery and see if the datasets are updated at the right time.&lt;/p&gt;
&lt;p&gt;At the moment the data is compiled to an R markdown document which makes anyone in our BI team to easily see if the data is up to date, should something be wrong, but use it in any way you see fit.&lt;/p&gt;
&lt;h3 id=&#34;the-service-account-key&#34;&gt;The service account key&lt;/h3&gt;
&lt;p&gt;I could spend a few hours writing down how to create a service account key in the Google Cloud Platform, however I think it would be smarter to link it right here instead: 
&lt;a href=&#34;https://cloud.google.com/iam/docs/creating-managing-service-account-keys&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://cloud.google.com/iam/docs/creating-managing-service-account-keys&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Once it is created, put it in the directory you are working on in R, and open it up. It should look like this:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;{
  &amp;quot;type&amp;quot;: &amp;quot;service_account&amp;quot;,
  &amp;quot;project_id&amp;quot;: &amp;quot;youproject&amp;quot;,
  &amp;quot;private_key_id&amp;quot;: &amp;quot;keyid1+239123&amp;quot;,
  &amp;quot;private_key&amp;quot;: &amp;quot;-----BEGIN PRIVATE KEY-----\nadfsdfsdfdsfdsfsdfaenIkd\nTVaeeeOZIODyN3ZYWxLwersffsdfdsb4we71lbHrg\nkJwHn8osrhssgfhCxp8y15YV8jrDof+TChTDe5wIj5WPvJO8\ndyBFchZx0ptbakmPb/ybeU7gKi3yPO29Mgoz4Cb47gNPvuZX82ic3dYE7YMEO1VF\nyfc1zHF6UJEAdrMSe+YO5oiGABemGj8FTNb+0Q0XurC4da3Qvwmz43OaICNTGzgI\nmkHB6fqsvSSVDi0agPE68wVGIDSEew9kEGUeAMw/e+T7NIY8z2PYJ5Dv6RWj/+b9\nh1XHiEwBAgMBAAECggEAISrTAqBfXFX966X8CVPjd3C198Yn/oyMssdfdsfdsÅQelFF\nJKxAt/Pnyr2xDYZLWgK3QChYibhufCwHq6V7BiWSO7F2PluuJedr9scG7u+t5se7\nr7gpyhTZIxljRsfdgsfgmEGDQz1wVpYnghySNHIHSWQcRG\nARYkv9v9LmwStA20Tr3BGEa1t3GF+4DAnW5DIP50TwzHv0wiG0+5vX3bWc+x/NR0\njAJKsDcoMzNtIqieLm7cizjS9Ku+rdHaelN8dt51jVAHATQ4oXwEyA0SpbkaLbqx\nvU+Y4U3plPZ1uAWHAtSI6fsrqOKpWA3g6MHVDiJQKQKBgQDGqbi62wq6vWETzByo\nOoiVvJLszm8iYA7LkgKPsJsfVvd+HJcv7uldL04ZaUIGvZ24shbRLN78Z0AnRzxX\nXZF3Vn64ROfxDSkcYOitipRdxBlsdfgrfbbZqeOvPVSy3qMn1Bqlf\ndFNvEtcMQ6YhRbAjbjgvlgiBTQKBgQC6yseYbmiJyRASBcyXcaguuvoebnn7l8r8\n2S1A7wTt1Sy5iPn1RfmUP4DK09y4zNtRYtNwNhu/VhB20MU8Na9ImKBVuvA3O0Ic\nIUeRFgDI2M3u4rGkLlsfgdgdf7f9oRUhPefGIAcNiqm3SACB2w9T\nuo5yMGIbhQKBgQCSkR7wBLCqysfdgfdgsdfO1X6U4kvCyG8oV1chsdfgfdgO6sSveufbbYGApKhpy2R6fydxNQKBgBHg\nØØØØØkzEA1exc3HfaaB5ayGz5LvLKP5hwe2Lt47HL6TFg\nw9t5Ui59jhXQ5Q9eE+hesvmBanver/UBOlgoQur11L+H8nhD9vEQOLOThyA2xmha\nS6V0sacUQ3Fq+0N6zTH+åååååøøøåå+q4y7JQSQiHd23\n22e4PAfX3Df+8860Axrx6fkImbvKxuOZ+uYpwhYMpPdgvukuAqXCHDcG5Fiho28g\n27JHFzQql0pn9Pj4O+WkUOs0rX9rcPjvV9h6Jc/YEWV6YwXMnEkKRaMj44+x7a69\npPfrTO7pBJAquiiU3dCmVDU=\n-----END PRIVATE KEY-----\n&amp;quot;,
  &amp;quot;client_email&amp;quot;: &amp;quot;yourserviceaccoutname@yourproject.iam.gserviceaccount.com&amp;quot;,
  &amp;quot;client_id&amp;quot;: &amp;quot;12345678910&amp;quot;,
  &amp;quot;auth_uri&amp;quot;: &amp;quot;https://accounts.google.com/o/oauth2/auth&amp;quot;,
  &amp;quot;token_uri&amp;quot;: &amp;quot;https://oauth2.googleapis.com/token&amp;quot;,
  &amp;quot;auth_provider_x509_cert_url&amp;quot;: &amp;quot;https://www.googleapis.com/oauth2/v1/certs&amp;quot;,
  &amp;quot;client_x509_cert_url&amp;quot;: &amp;quot;https://www.googleapis.com/robot/v1/metadata/x509/yourserviceaccountname%40yourproject.iam.gserviceaccount.com&amp;quot;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here you need to add the email you see under &lt;code&gt; &amp;quot;client_email&amp;quot;: &amp;quot;yourserviceaccoutname@yourproject.iam.gserviceaccount.com&amp;quot;&lt;/code&gt; and add that to all the BigQuery projects you want to monitor:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;iamaccess.png&#34; alt=&#34;IAM ACESS&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;building-the-code&#34;&gt;Building the code&lt;/h3&gt;
&lt;h5 id=&#34;1-authenticate-and-load-libraries&#34;&gt;1. authenticate and load libraries**&lt;/h5&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#libraries
library(bigQueryR)
library(purrr)
library(dplyr)

#auth
bigQueryR::bqr_auth(&amp;quot;yourauthfile.json&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;2-get-a-list-of-all-projects-and-datasets&#34;&gt;2. Get a list of all projects and datasets&lt;/h5&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#get projects
projects &amp;lt;- bigQueryR::bqr_list_projects()
#get datasets
datasets &amp;lt;- lapply(projects$projectId,bqr_list_datasets)
datasets &amp;lt;- bind_rows(datasets)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This will give us two tables that show us all the projects and datasets that the service account have access to. We need this information to list all avaliable tables.&lt;/p&gt;
&lt;h5 id=&#34;3-get-a-list-of-all-tables-to-extract-metadata&#34;&gt;3. Get a list of all tables to extract metadata&lt;/h5&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#get tables
listTables &amp;lt;- function(id,dataset){
bqr_list_tables(projectId = id,
                datasetId = dataset, maxResults = -1)
}

tables &amp;lt;- mapply(listTables,datasets$projectId,datasets$datasetId)
tablesCombined &amp;lt;- bind_rows(tables)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then we can use these lines in order to extract the metadata:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#get meta
tablesMeta &amp;lt;- function(project,dataset,table){
bqr_table_meta(projectId = project,
               datasetId = dataset, table)
}

tablesMeta &amp;lt;- mapply(tablesMeta,tablesCombined$projectId,tablesCombined$datasetId,tablesCombined$tableId)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It can take some time if you have a lot of projects. If you want to speed up the process you can process it with the future package instead to run it in parallel: 
&lt;a href=&#34;https://cran.r-project.org/web/packages/future/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://cran.r-project.org/web/packages/future/index.html&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Once it is done, we get a list of which gives us a list of objects with metadata:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;bqmeta.png&#34; alt=&#34;BQ META&#34;&gt;&lt;/p&gt;
&lt;p&gt;As seen, it isn&amp;rsquo;t great for working with yet, so we need to take the information we need for each list object and stitch it together in a format that is more fit for working with.&lt;/p&gt;
&lt;p&gt;The process will be to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Get the number of rows in the listobject&lt;/li&gt;
&lt;li&gt;create an empty dataframe with column names&lt;/li&gt;
&lt;li&gt;extract the data we need for each list&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;#create a dataframe with the right information for bq
#get the total rows
i &amp;lt;- seq_along(1:NROW(tablesMeta)) 
#create an empty dataframe with column headers
bqUpdate &amp;lt;- setNames(data.frame(matrix(ncol = 7, nrow = 0)), c(&amp;quot;id&amp;quot;,&amp;quot;projectId&amp;quot;,&amp;quot;datasetId&amp;quot;,&amp;quot;creationTime&amp;quot;,&amp;quot;lastModifiedTime&amp;quot;,&amp;quot;type&amp;quot;,&amp;quot;location&amp;quot;))

#create function for looping for appending metadata
combineMetaData &amp;lt;- function(i){
add_row(bqUpdate,
       id = tablesMeta[[i]][[&amp;quot;id&amp;quot;]],
       projectId = tablesMeta[[i]][[&amp;quot;tableReference&amp;quot;]][[&amp;quot;projectId&amp;quot;]],
       datasetId = tablesMeta[[i]][[&amp;quot;tableReference&amp;quot;]][[&amp;quot;datasetId&amp;quot;]],
       creationTime = tablesMeta[[i]][[&amp;quot;creationTime&amp;quot;]],
       lastModifiedTime = tablesMeta[[i]][[&amp;quot;lastModifiedTime&amp;quot;]],
       type = tablesMeta[[i]][[&amp;quot;type&amp;quot;]],
       location = tablesMeta[[i]][[&amp;quot;location&amp;quot;]]
       )
}

#run the function and combine it
upload &amp;lt;- map(i,combineMetaData)
upload &amp;lt;- bind_rows(upload)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally format the table a little, so it is more readable:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;upload$creationTime &amp;lt;- format(as.POSIXct(as.double(upload$creationTime)/1000, origin = &amp;quot;1970-01-01&amp;quot;, tz = &amp;quot;GMT-1&amp;quot;),&amp;quot;%Y-%m-%d %H:%M:%S&amp;quot; )
upload$lastModifiedTime &amp;lt;- format(as.POSIXct(as.double(upload$lastModifiedTime)/1000, origin = &amp;quot;1970-01-01&amp;quot;, tz = &amp;quot;GMT-1&amp;quot;),&amp;quot;%Y-%m-%d %H:%M:%S&amp;quot; )
upload$dataset &amp;lt;- gsub(&amp;quot;.*\\.&amp;quot;,&amp;quot;&amp;quot;,upload$id) 
upload$daysSinceUpdate &amp;lt;- Sys.Date()-as.Date(as.character(upload$lastModifiedTime), format=&amp;quot;%Y-%m-%d&amp;quot;)
upload &amp;lt;- upload[order(upload$daysSinceUpdate, decreasing=F),]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The output should look something like this:
&lt;img src=&#34;bqjobdone.png&#34; alt=&#34;bqtableextracted&#34;&gt;&lt;/p&gt;
&lt;p&gt;And voila, you now have a list of all your tables in BigQuery, as well as being able to see how many days it is since they were last updated.&lt;/p&gt;
&lt;h3 id=&#34;optional-r-markdown&#34;&gt;Optional R Markdown&lt;/h3&gt;
&lt;p&gt;In our company, the persons in charge of marketing automation and data visualization, wanted to be able to pinpoint if there should be any issues with having updated data in their systems, as the first thing they look through. I therefore created a 
&lt;a href=&#34;https://rmarkdown.rstudio.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;R markdown document&lt;/a&gt;, that they could use by visiting a page stored in 
&lt;a href=&#34;https://cloud.google.com/storage/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Google Cloud Storage&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;mqmd.png&#34; alt=&#34;bqmd&#34;&gt;&lt;/p&gt;
&lt;p&gt;This makes it possible to see the tables who are not updated within the last day, which one who are updated, and a list of all tables. It even colors table in different colors depending on how long it has been since the last update.&lt;/p&gt;
&lt;p&gt;Tables not updated are not nesscesarily a bad thing, as some static tables are needed to benchmark. I do however recommend to look at some of the tables once in a while to see if it would make sense to delete them.&lt;/p&gt;
&lt;p&gt;Below is the code that I used to generate it:&lt;/p&gt;
&lt;script src=&#34;https://gist.github.com/dannymawani/a38d2a5717a89e7218f6d91c7f465bdc.js&#34;&gt;&lt;/script&gt;
&lt;h3 id=&#34;conclusion-and-final-thoughts&#34;&gt;Conclusion and final thoughts&lt;/h3&gt;
&lt;p&gt;This post displays a way to monitor your bigQuery tables. If you are running a lot of jobs, it can make sense to use this as a quick way to see if some tables are not updated, should some of your systems fail. For building data pipelines using cronjobs, og cloud builds etc. I do recommend take a look at 
&lt;a href=&#34;https://cloud.google.com/products/operations&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Stackdriver in GCP&lt;/a&gt;, as it can monitor failed jobs and send textmessages, slack notifications or emails with customized dynamic text. I will try and write a guide about this in the future.&lt;/p&gt;
&lt;p&gt;If you do consider automating grabbing the metadata from bigQuery, I recommend you to read 
&lt;a href=&#34;https://code.markedmondson.me/googleCloudRunner/articles/usecases.html#build-an-rmd-on-a-schedule-and-host-its-html-on-cloud-storage&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Mark&amp;rsquo;s post on how to update it automatically using Google Cloud Build&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Let me know if this post is easy to follow or if it needs additional explanation by leaving a comment.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>R on the Google Cloud Platform - Scheduling tasks</title>
      <link>/talk/r-on-google-cloud-platform/</link>
      <pubDate>Sat, 08 Jun 2019 00:00:00 +0000</pubDate>
      <guid>/talk/r-on-google-cloud-platform/</guid>
      <description>&lt;p&gt;&lt;strong&gt;the slides&lt;/strong&gt;&lt;/p&gt;
&lt;div style=&#34;position: relative; padding-bottom: 80.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;../../slides/r-on-gcp-slides&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; webkitallowfullscreen mozallowfullscreen allowfullscreen&gt;&lt;/iframe&gt;
 &lt;/div&gt;</description>
    </item>
    
    <item>
      <title>MEASURECAMP 2019 - CPH</title>
      <link>/slides/r-on-gcp-slides/</link>
      <pubDate>Tue, 05 Feb 2019 00:00:00 +0000</pubDate>
      <guid>/slides/r-on-gcp-slides/</guid>
      <description>&lt;h1 id=&#34;measurecamp-cph19&#34;&gt;Measurecamp CPH19&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;Creating datapipelines with R&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;By Danny Mawani Holmgaard&lt;/em&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;agenda&#34;&gt;Agenda&lt;/h2&gt;
&lt;div style=&#34;float: left; width: 50%; text-align: left;&#34;&gt;
&lt;ol&gt;
&lt;li&gt;About my journey&lt;/li&gt;
&lt;li&gt;Stuff I am still figuring out&lt;/li&gt;
&lt;li&gt;Learn to grab different types of data&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div style=&#34;float: right; width: 50%; text-align: left;&#34;&gt;
&lt;ol start=&#34;4&#34;&gt;
&lt;li&gt;Connect Github and Cloud Storage&lt;/li&gt;
&lt;li&gt;Use R in the GCP&lt;/li&gt;
&lt;li&gt;Extract Transform and Load data into BigQuery&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;h3 id=&#34;my-journey&#34;&gt;My journey&lt;/h3&gt;
&lt;div style=&#34;float: left; width: 50%; text-align: left;&#34;&gt;
&lt;ul&gt;
&lt;li&gt;Worked with Digital Analytics for 6 years&lt;/li&gt;
&lt;li&gt;Lead Analyst @ IMPACT EXTEND&lt;/li&gt;
&lt;li&gt;Responsible for all data pipelines&lt;/li&gt;
&lt;li&gt;Moving from digital analytics to data engineering&lt;/li&gt;
&lt;li&gt;Primarily use R and GCP&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div style=&#34;float: right; width: 50%; text-align: left;&#34;&gt;
&lt;p&gt;&lt;img src=&#34;superweek.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;h2 id=&#34;our-team&#34;&gt;Our team&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;workstuff.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;stuff-i-am-currently-trying-to-get-better-at-using&#34;&gt;Stuff I am currently trying to get better at using&lt;/h3&gt;
&lt;div style=&#34;float: left; width: 60%; text-align: left;&#34;&gt;
&lt;ul&gt;
&lt;li&gt;Using Python and SQL instead of using R for everything&lt;/li&gt;
&lt;li&gt;Utilizing Docker&lt;/li&gt;
&lt;li&gt;Set up plumber API&amp;rsquo;s to make my life easier&lt;/li&gt;
&lt;li&gt;Deploying shinyapps to GCP with authentication&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div style=&#34;float: right; width: 40%; text-align: left;&#34;&gt;
&lt;p&gt;&lt;img src=&#34;docker.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;h3 id=&#34;what-i-am-trying-to-say-is-that-there-is-a-lot-to-grasp-and-that-it-takes-time-to-optimize-your-workflow&#34;&gt;What i am trying to say is that there is a lot to grasp and that it takes time to optimize your workflow&lt;/h3&gt;
&lt;hr&gt;
&lt;h3 id=&#34;but-now-lets-get-to-the-place-where-we-can-get-to-do-some-cool-stuff&#34;&gt;But now, lets get to the place where we can get to do some cool stuff!&lt;/h3&gt;
&lt;hr&gt;
&lt;h1 id=&#34;grabbing-different-types-of-data&#34;&gt;Grabbing different types of data&lt;/h1&gt;
&lt;hr&gt;
&lt;h3 id=&#34;sql-servers&#34;&gt;SQL Servers&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#load libraries
#library(tidyverse)
#library(dbi)
#library(odbc)

#set up connection object
con &amp;lt;- DBI::dbConnect(odbc::odbc(),
                      Driver    = &amp;quot;SQL Server&amp;quot;, 
                      Server    = &amp;quot;yourserver.database.windows.net&amp;quot;,
                      Database  = &amp;quot;databasename&amp;quot;,
                      UID       = &amp;quot;userid&amp;quot;,
                      PWD       = &amp;quot;password&amp;quot;,
                      Port      = 1234)

#extract data
dataset &amp;lt;- as_tibble(
  tbl(con, &amp;quot;dataset&amp;quot;)%&amp;gt;%
  head(100) #Get the first 100 rows
  ) 

&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h3 id=&#34;ftp-servers&#34;&gt;FTP Servers&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#library(RCurl)
syntax &amp;lt;- &amp;quot;ftp://&amp;quot;
ftpHost &amp;lt;- &amp;quot;ftp.yourserver.com.com&amp;quot;
ftpUser &amp;lt;- &amp;quot;username&amp;quot;
ftpPass &amp;lt;- &amp;quot;password&amp;quot;
folder &amp;lt;- &amp;quot;/folder/&amp;quot; #delete folder if in root
ftpURL &amp;lt;- paste(syntax,ftpHost,folder,sep = &amp;quot;&amp;quot;)

#Download files function

download &amp;lt;- function(file){
  fileDownload &amp;lt;- file
  downloadFtpUrl &amp;lt;- paste(ftpURL,fileDownload, sep = &amp;quot;&amp;quot;)
  downloadFtpUrlCredentials &amp;lt;- paste(ftpUser,&amp;quot;:&amp;quot;,ftpPass, sep = &amp;quot;&amp;quot;)
  bin &amp;lt;- getBinaryURL(downloadFtpUrl,userpwd=downloadFtpUrlCredentials)
  con &amp;lt;- file(fileDownload, open = &amp;quot;wb&amp;quot;)
  writeBin(bin, con)
  close(con)}

#Download files

download(&amp;quot;yourfile.csv&amp;quot;)

&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;postgress-database&#34;&gt;postgress database&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#library(&#39;RPostgreSQL&#39;)

#create connection object
con &amp;lt;- dbConnect(drv =&amp;quot;RPostgreSQL&amp;quot;, 
                 user=&amp;quot;&amp;quot;, 
                 password=&amp;quot;&amp;quot;,
                 host=&amp;quot;&amp;quot;, 
                 port=1234, 
                 dbname=&amp;quot;&amp;quot;)

#extract data
dataset &amp;lt;- as_tibble(
  tbl(con, &amp;quot;dataset&amp;quot;)%&amp;gt;%
  head(100) #Get the first 100 rows
  ) 

&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;google-analytics&#34;&gt;Google Analytics&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;ga.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;facebook-and-instagram&#34;&gt;Facebook and Instagram&lt;/h3&gt;
&lt;div style=&#34;float: left; width: 50%; text-align: left;&#34;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;It is difficult to download Facebook and instagram data&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Seek a third party vendor instead&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div style=&#34;float: right; width: 50%; text-align: right;&#34;&gt;
&lt;ul&gt;
&lt;li&gt;Remember, maintaining API work can require a lof of work and could potentially break whereas a vendor are living of maintaining these tools and have the right types of access&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;p&gt;&lt;img src=&#34;stitch.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;connect-github-and-cloud-storage-to-your-work&#34;&gt;Connect Github and Cloud storage to your work&lt;/h2&gt;
&lt;hr&gt;
&lt;h3 id=&#34;github&#34;&gt;Github&lt;/h3&gt;
&lt;div style=&#34;float: left; width: 60%; text-align: right;&#34;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Github is the closest thing you get to a &amp;ldquo;dropbox&amp;rdquo; for your code.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;It allows version control and makes sure that your code is always updated&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;From github, you can push you code into other systems and work on the same projects&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div style=&#34;float: right; width: 40%; text-align: right;&#34;&gt;
&lt;p&gt;&lt;img src=&#34;github.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;h2 id=&#34;cloud-storage&#34;&gt;Cloud Storage&lt;/h2&gt;
&lt;div style=&#34;float: left; width: 60%; text-align: right;&#34;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Cloud storage lets you upload and pull files in a secure environment&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;It has great API&amp;rsquo;s and can sync directly with bigQuery&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Thanks to Mark Edmonson, we can also source R code directly from there&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div style=&#34;float: right; width: 40%; text-align: right;&#34;&gt;
&lt;p&gt;&lt;img src=&#34;cloudstorage.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div style=&#34;float: left; width: 50%; text-align: right;&#34;&gt;
&lt;ol&gt;
&lt;li&gt;Set up Github and put you code there&lt;/li&gt;
&lt;li&gt;Create a bucket in Cloud Storage&lt;/li&gt;
&lt;li&gt;Create a trigger with cloudbuild where you link to the repo&lt;/li&gt;
&lt;li&gt;Add cloudbuild.yalm to the folder&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div style=&#34;float: right; width: 50%; text-align: right;&#34;&gt;
&lt;pre&gt;&lt;code class=&#34;language-javaScript&#34;&gt;steps:
  - name: gcr.io/cloud-builders/gsutil
    args: [&amp;quot;-m&amp;quot;, &amp;quot;rsync&amp;quot;, &amp;quot;-r&amp;quot;, &amp;quot;-c&amp;quot;, &amp;quot;-d&amp;quot;, &amp;quot;.&amp;quot;, &amp;quot;gs://yourfolderincloudstorage/ifneededthencreateasubfolder&amp;quot;]
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;h2 id=&#34;sourcing-from-cloud-storage&#34;&gt;Sourcing from Cloud Storage&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;
#install.package(&amp;quot;googleCloudStorageR&amp;quot;) if you have not installed it yet

#load the library
library(googleCloudStorageR)

#Authenticate
gcs_auth()

googleCloudStorageR::gcs_source(&#39;yoursubfolderifyouhaveone/yourscript.R&#39;, bucket = &#39;yourcreatedbucket&#39;)
                 
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h1 id=&#34;use-r-studio-in-the-google-cloud-platform&#34;&gt;Use R studio in the Google Cloud Platform&lt;/h1&gt;
&lt;hr&gt;
&lt;h2 id=&#34;1-set-up-billing&#34;&gt;1. Set up billing&lt;/h2&gt;
&lt;hr&gt;
&lt;h2 id=&#34;2-thank-mark-that-he-build-a-script-that-does-everything-for-you&#34;&gt;2. Thank Mark that he build a script that does everything for you&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# You need to authenticate with your GCP account before being able to do it
gce_vm(&amp;quot;yourmachinename&amp;quot;, project =&amp;quot;gar-creds-185213&amp;quot;, zone = &amp;quot;europe-west1-b&amp;quot;,
       predefined_type = &amp;quot;g1-small&amp;quot;,
       template = &amp;quot;rstudio&amp;quot;, 
       username = &amp;quot;username&amp;quot;, 
       password = &amp;quot;password&amp;quot;)

&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;the-easy-way-to-run-scripts-automatically-from-gcs&#34;&gt;The easy way to run scripts automatically from GCS&lt;/h2&gt;
&lt;div style=&#34;float: left; width: 50%; text-align: right;&#34;&gt;
&lt;p&gt;&lt;img src=&#34;rstudiogcp.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div style=&#34;float: right; width: 50%; text-align: right;&#34;&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#set machine to be launched
library(googleComputeEngineR)
library(googleCloudStorageR)
library(googleAuthR)

gar_auth(&amp;quot;/home/username/.httr-oauth&amp;quot;)

GCE_AUTH_FILE=&amp;quot;/home/username/auth.json&amp;quot;
GCE_DEFAULT_PROJECT_ID=&amp;quot;projectname&amp;quot;
GCE_DEFAULT_ZONE=&amp;quot;europe-west1-b&amp;quot;
gcs_global_bucket(&amp;quot;bucketname&amp;quot;)
BQ_AUTH_FILE=&amp;quot;/home/username/bq.oauth&amp;quot;

vm &amp;lt;- gce_vm(&amp;quot;yourvirtualmachine&amp;quot;)

vm &amp;lt;- gce_ssh_setup(vm,
                    username = &amp;quot;username&amp;quot;,
                    key.pub = &amp;quot;/home/username/.ssh/id_rsa.pub&amp;quot;,
                    key.private = &amp;quot;/home/username/.ssh/id_rsa&amp;quot;)

runme &amp;lt;- &amp;quot;Rscript -e \&amp;quot;googleAuthR::gar_gce_auth();googleCloudStorageR::gcs_source(&#39;cloudstoragefolder/script.r&#39;, bucket = &#39;bucket&#39;)\&amp;quot;&amp;quot;
docker_cmd(vm, 
           cmd = &amp;quot;exec&amp;quot;, 
           args = c(&amp;quot;rstudio&amp;quot;, runme), 
           wait = TRUE,
           capture_text = FALSE)

&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;h3 id=&#34;automating-the-script-to-run-at-a-specific-time&#34;&gt;Automating the script to run at a specific time&lt;/h3&gt;
&lt;h4 id=&#34;remember---your-if-you-turn-of-the-machine-the-cronjob-settings-will-stop-working&#34;&gt;&lt;em&gt;Remember - your if you turn of the machine, the cronjob settings will stop working&lt;/em&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img src=&#34;cronjobs.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;sending-data-to-bigquery&#34;&gt;Sending data to bigQuery&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#library(bigQueryR)

# First Create the dataset

# bqr_create_table(projectId = &amp;quot;your project id&amp;quot;,
#                  datasetId = &amp;quot;dataset&amp;quot;, &amp;quot;your table&amp;quot;, your dataframe,
#                  timePartitioning = FALSE, expirationMs = 0L)

bqr_upload_data(projectId = &amp;quot;your project id&amp;quot;,
                datasetId = &amp;quot;dataset&amp;quot;, &amp;quot;your table&amp;quot;, yourdataframe,
                overwrite = FALSE, #True to overwrite your table
                wait = TRUE, autodetect = FALSE,
                maxBadRecords = 1000)

&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;final-thoughts&#34;&gt;Final thoughts&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;This is not the most stable way to do things, but the easiest&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;There is so many ways you can work with making your data flow&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Start small and build your way up from there&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;h2 id=&#34;questions&#34;&gt;Questions?&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;@dannymawani (Twitter / Linkedin)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;http://www.mawani.dk&#34;&gt;www.mawani.dk&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;mailto:dmo@impact.dk&#34;&gt;dmo@impact.dk&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Superweek 2019 - A tale of automation in data engineering</title>
      <link>/talk/superweek-2019/</link>
      <pubDate>Fri, 01 Feb 2019 00:00:00 +0000</pubDate>
      <guid>/talk/superweek-2019/</guid>
      <description>&lt;p&gt;&lt;strong&gt;the slides&lt;/strong&gt;&lt;/p&gt;
&lt;iframe src=&#34;//www.slideshare.net/slideshow/embed_code/key/28qsnfrw9J3zHF&#34; width=&#34;595&#34; height=&#34;485&#34; frameborder=&#34;0&#34; marginwidth=&#34;0&#34; marginheight=&#34;0&#34; scrolling=&#34;no&#34; style=&#34;border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;&#34; allowfullscreen&gt; &lt;/iframe&gt; &lt;div style=&#34;margin-bottom:5px&#34;&gt; &lt;strong&gt; &lt;a href=&#34;//www.slideshare.net/DannyMawaniOlsen/superweek2019-dmo-presentation&#34; title=&#34;Superweek2019 dmo presentation&#34; target=&#34;_blank&#34;&gt;Superweek2019 dmo presentation&lt;/a&gt; &lt;/strong&gt; from &lt;strong&gt;&lt;a href=&#34;https://www.slideshare.net/DannyMawaniOlsen&#34; target=&#34;_blank&#34;&gt;Danny Mawani Olsen&lt;/a&gt;&lt;/strong&gt; &lt;/div&gt;</description>
    </item>
    
    <item>
      <title>Black friday seminar</title>
      <link>/talk/blackfriday/</link>
      <pubDate>Sat, 01 Dec 2018 10:00:00 +0000</pubDate>
      <guid>/talk/blackfriday/</guid>
      <description>&lt;h1 id=&#34;se-videoen-her&#34;&gt;Se videoen her:&lt;/h1&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/LNRdiEFOtMQ&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

</description>
    </item>
    
    <item>
      <title>GDPR with Google Tag Manager</title>
      <link>/talk/ip-anonymization-while-excluding-internal-trafic/</link>
      <pubDate>Sat, 01 Dec 2018 00:00:00 +0000</pubDate>
      <guid>/talk/ip-anonymization-while-excluding-internal-trafic/</guid>
      <description>&lt;iframe src=&#34;//www.slideshare.net/slideshow/embed_code/key/8L7n7nX0ZUINKB&#34; width=&#34;595&#34; height=&#34;485&#34; frameborder=&#34;0&#34; marginwidth=&#34;0&#34; marginheight=&#34;0&#34; scrolling=&#34;no&#34; style=&#34;border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;&#34; allowfullscreen&gt; &lt;/iframe&gt; &lt;div style=&#34;margin-bottom:5px&#34;&gt; &lt;strong&gt; &lt;a href=&#34;//www.slideshare.net/DannyMawaniOlsen/gdpr-within-google-tag-manager-measurecamp-2018&#34; title=&#34;GDPR within Google Tag Manager - Measurecamp 2018&#34; target=&#34;_blank&#34;&gt;GDPR within Google Tag Manager - Measurecamp 2018&lt;/a&gt; &lt;/strong&gt; from &lt;strong&gt;&lt;a href=&#34;https://www.slideshare.net/DannyMawaniOlsen&#34; target=&#34;_blank&#34;&gt;Danny Mawani Olsen&lt;/a&gt;&lt;/strong&gt; &lt;/div&gt;</description>
    </item>
    
    <item>
      <title>How to anonymize IP adresses and still be able to exclude internal traffic</title>
      <link>/talk/gdpr-with-google-tag-manager/</link>
      <pubDate>Sat, 01 Dec 2018 00:00:00 +0000</pubDate>
      <guid>/talk/gdpr-with-google-tag-manager/</guid>
      <description>&lt;p&gt;
&lt;a href=&#34;https://analytics.mawani.dk/excluding-internal-traffic-anonymizing-ip-adresses-google-analytics/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Original Link&lt;/a&gt;&lt;/p&gt;
&lt;iframe src=&#34;//www.slideshare.net/slideshow/embed_code/key/6Vr0jPcBgPD6dd&#34; width=&#34;595&#34; height=&#34;485&#34; frameborder=&#34;0&#34; marginwidth=&#34;0&#34; marginheight=&#34;0&#34; scrolling=&#34;no&#34; style=&#34;border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;&#34; allowfullscreen&gt; &lt;/iframe&gt; &lt;div style=&#34;margin-bottom:5px&#34;&gt; &lt;strong&gt; &lt;a href=&#34;//www.slideshare.net/DannyMawaniOlsen/anonymization-of-ip-adresses-with-google-tag-manager&#34; title=&#34;Anonymization of IP adresses with Google Tag Manager&#34; target=&#34;_blank&#34;&gt;Anonymization of IP adresses with Google Tag Manager&lt;/a&gt; &lt;/strong&gt; from &lt;strong&gt;&lt;a href=&#34;https://www.slideshare.net/DannyMawaniOlsen&#34; target=&#34;_blank&#34;&gt;Danny Mawani Olsen&lt;/a&gt;&lt;/strong&gt; &lt;/div&gt;</description>
    </item>
    
    <item>
      <title>R introduction class for measurecamp</title>
      <link>/talk/r-introduction-class-measurecamp/</link>
      <pubDate>Sat, 01 Dec 2018 00:00:00 +0000</pubDate>
      <guid>/talk/r-introduction-class-measurecamp/</guid>
      <description>&lt;p&gt;&lt;strong&gt;the slides&lt;/strong&gt;&lt;/p&gt;
&lt;iframe src=&#34;//www.slideshare.net/slideshow/embed_code/key/uz2aLKQKZ2CSiB&#34; width=&#34;595&#34; height=&#34;485&#34; frameborder=&#34;0&#34; marginwidth=&#34;0&#34; marginheight=&#34;0&#34; scrolling=&#34;no&#34; style=&#34;border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;&#34; allowfullscreen&gt; &lt;/iframe&gt; &lt;div style=&#34;margin-bottom:5px&#34;&gt; &lt;strong&gt; &lt;a href=&#34;//www.slideshare.net/DannyMawaniOlsen/rclass&#34; title=&#34;Rclass&#34; target=&#34;_blank&#34;&gt;Rclass&lt;/a&gt; &lt;/strong&gt; from &lt;strong&gt;&lt;a href=&#34;https://www.slideshare.net/DannyMawaniOlsen&#34; target=&#34;_blank&#34;&gt;Danny Mawani Olsen&lt;/a&gt;&lt;/strong&gt; &lt;/div&gt;
&lt;p&gt;
&lt;a href=&#34;https://documentcloud.adobe.com/link/track?uri=urn:aaid:scds:US:a2aa2406-42b7-4030-8bb9-fa53525d20c8&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;full link here&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
